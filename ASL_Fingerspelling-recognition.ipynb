{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# American Sign Language Fingerspelling recognition","metadata":{"papermill":{"duration":0.023581,"end_time":"2023-07-02T11:32:59.503233","exception":false,"start_time":"2023-07-02T11:32:59.479652","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install mediapipe","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:34:04.484702Z","iopub.execute_input":"2023-09-24T16:34:04.485073Z","iopub.status.idle":"2023-09-24T16:34:19.061706Z","shell.execute_reply.started":"2023-09-24T16:34:04.485041Z","shell.execute_reply":"2023-09-24T16:34:19.060539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sn\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split, GroupShuffleSplit\nfrom leven import levenshtein\n\nimport glob\nimport sys\nimport random\nimport os\nimport math\nimport gc\nimport sys\nimport sklearn\nimport time\nimport json\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport matplotlib \nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport os\nimport math\nimport gc\nimport shutil\nimport pyarrow.parquet as pq\nimport json\nimport mediapipe\nimport random\nimport json\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom skimage.transform import resize\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm\nfrom matplotlib import animation, rc\nfrom mediapipe.framework.formats import landmark_pb2\nfrom tensorflow.keras.callbacks import Callback\n\n# TQDM Progress Bar With Pandas Apply Function\ntqdm.pandas()\n\nprint(f'Tensorflow Version {tf.__version__}')\nprint(f'Python Version: {sys.version}')","metadata":{"papermill":{"duration":9.205219,"end_time":"2023-07-02T11:33:08.731341","exception":false,"start_time":"2023-07-02T11:32:59.526122","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:34:19.064109Z","iopub.execute_input":"2023-09-24T16:34:19.064496Z","iopub.status.idle":"2023-09-24T16:34:27.704073Z","shell.execute_reply.started":"2023-09-24T16:34:19.064459Z","shell.execute_reply":"2023-09-24T16:34:27.701789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the training dataset from 'train.csv' and display the shape of the training dataset.","metadata":{}},{"cell_type":"code","source":"dataset_df = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\nprint(\"Full train dataset shape is {}\".format(dataset_df.shape))","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:34:27.705352Z","iopub.execute_input":"2023-09-24T16:34:27.706575Z","iopub.status.idle":"2023-09-24T16:34:27.885291Z","shell.execute_reply.started":"2023-09-24T16:34:27.706513Z","shell.execute_reply":"2023-09-24T16:34:27.884311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:34:27.887987Z","iopub.execute_input":"2023-09-24T16:34:27.888888Z","iopub.status.idle":"2023-09-24T16:34:27.906309Z","shell.execute_reply.started":"2023-09-24T16:34:27.888853Z","shell.execute_reply":"2023-09-24T16:34:27.905287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Character 2 Number Encoding","metadata":{"papermill":{"duration":0.022966,"end_time":"2023-07-02T11:33:08.779005","exception":false,"start_time":"2023-07-02T11:33:08.756039","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Reading Character-to-Number Encoding Mapping\n\n#### Reading JSON File\n- The code opens a JSON file located at `/kaggle/input/asl-fingerspelling/character_to_prediction_index.json`.\n\n#### Loading Character-to-Number Mapping\n- The content of the JSON file is loaded into a Python dictionary named `char_to_num`. This dictionary represents a character-to-number encoding mapping, where characters are keys and numbers are values.\n\n#### Creating Number-to-Character Mapping\n- The code creates a new dictionary named `num_to_char` by reversing the key-value pairs of `char_to_num`. In this dictionary, numbers become keys, and characters become values.\n\n#### Converting Dictionary to DataFrame\n- A pandas DataFrame named `char_to_num_df` is created from the `char_to_num` dictionary. This DataFrame has two columns: 'Number Encoding' and 'Character', where 'Character' corresponds to the keys from `char_to_num`, and 'Number Encoding' corresponds to the values.\n\n#### Displaying the First Few Rows\n- The `head()` method is used to display the first few rows of the `char_to_num_df` DataFrame.\n\nThis part helpful for working with character-to-number encoding mappings and allows you to easily convert between characters and their numeric representations using pandas DataFrames.\n","metadata":{}},{"cell_type":"code","source":"# Read Character to Number Encoding Mapping\nwith open('/kaggle/input/asl-fingerspelling/character_to_prediction_index.json') as json_file:\n    char_to_num = json.load(json_file)\n    \n# Number to Character Mapping\nnum_to_char = {j:i for i,j in char_to_num.items()}\n\n# convert dictionary to pandas dataframe\nchar_to_num_df = pd.DataFrame(char_to_num.values(),index=char_to_num.keys(),columns=['Number Encoding'])\nchar_to_num_df.head()","metadata":{"papermill":{"duration":0.057156,"end_time":"2023-07-02T11:33:08.859134","exception":false,"start_time":"2023-07-02T11:33:08.801978","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:34:27.907754Z","iopub.execute_input":"2023-09-24T16:34:27.908257Z","iopub.status.idle":"2023-09-24T16:34:27.925787Z","shell.execute_reply.started":"2023-09-24T16:34:27.908224Z","shell.execute_reply":"2023-09-24T16:34:27.924484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration and Settings\n\n#### Environment Check\n- The code checks the environment to determine if the notebook is running interactively or as a commit on Kaggle. It sets the `IS_INTERACTIVE` variable accordingly. You can manually set it to `False` if you want to run the full code regardless of the environment.\n\n#### Verbosity Setting\n- The `VERBOSE` variable is used to control the level of verbosity during training. If the notebook is run interactively, it sets verbosity to 1 for more detailed output. Otherwise, it sets verbosity to 2.\n\n#### Global Random Seed\n- The `SEED` variable sets a global random seed to ensure that random operations are reproducible across runs.\n\n#### Number of Target Frames\n- `N_TARGET_FRAMES` specifies the number of frames to which recordings should be resized.\n\n#### Debug Mode\n- `DEBUG` is a debug flag. When set to `True`, it enables a debug mode that takes a subset of the training data for debugging purposes.\n\n#### Number of Unique Characters and Special Tokens\n- `N_UNIQUE_CHARACTERS0` represents the number of unique characters in your data before adding special tokens.\n- `N_UNIQUE_CHARACTERS` represents the total number of unique characters, including padding, start of sentence, and end of sentence tokens.\n- `PAD_TOKEN`, `START_TOKEN`, `END_TOKEN` hold token values for padding, start of sentence, and end of sentence, respectively.\n\n#### Validation Data\n- `USE_VAL` determines whether 10% of the data should be used for validation during training.\n\n#### Batch Size\n- `BATCH_SIZE` specifies the batch size for training.\n\n#### Number of Epochs\n- `N_EPOCHS` defines the number of training epochs. It's set to 2 if running interactively; otherwise, it's set to 30.\n\n#### Warmup Epochs\n- `N_WARMUP_EPOCHS` specifies the number of warm-up epochs for the learning rate scheduler.\n\n#### Maximum Learning Rate\n- `LR_MAX` sets the maximum learning rate for the optimizer.\n\n#### Weight Decay Ratio\n- `WD_RATIO` represents the weight decay ratio as a fraction of the learning rate.\n\n#### Maximum Phrase Length\n- `MAX_PHRASE_LENGTH` is the maximum length of a phrase plus the end of sentence token.\n\n#### Training and Loading Weights\n- `TRAIN_MODEL` indicates whether the model should be trained.\n- `LOAD_WEIGHTS` specifies whether pretrained weights should be loaded.\n\n#### Learning Rate Warmup Method\n- `WARMUP_METHOD` defines the learning rate warmup method, which can be 'log' or 'exp'.\n\nThese settings can be adjusted to customize the behavior of the training and model based on specific requirements.","metadata":{"papermill":{"duration":0.022903,"end_time":"2023-07-02T11:33:08.905507","exception":false,"start_time":"2023-07-02T11:33:08.882604","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# If Notebook Is Run By Committing or In Interactive Mode For Development\n#IS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\nIS_INTERACTIVE = False # to run full code\n\n# Verbose Setting during training\nVERBOSE = 1 if IS_INTERACTIVE else 2\n\n# Global Random Seed\nSEED = 42\n\n# Number of Frames to resize recording to\nN_TARGET_FRAMES = 128\n\n# Global debug flag, takes subset of train\nDEBUG = False\n\n# Number of Unique Characters To Predict + Pad Token + SOS Token + EOS Token\nN_UNIQUE_CHARACTERS0 = len(char_to_num)\nN_UNIQUE_CHARACTERS = len(char_to_num) + 1 + 1 + 1\nPAD_TOKEN = len(char_to_num) # Padding # 59\nSTART_TOKEN = len(char_to_num) + 1 # Start Of Sentence # 60\nEND_TOKEN = len(char_to_num) + 2 # End Of Sentence # 61\n\n# Whether to use 10% of data for validation\nUSE_VAL = True\n\n# Batch Size\nBATCH_SIZE = 64\n\n# Number of Epochs to Train for\nN_EPOCHS = 2 if IS_INTERACTIVE else 50\n\n# Number of Warmup Epochs in Learning Rate Scheduler\nN_WARMUP_EPOCHS = 5\n\n# Maximum Learning Rate\nLR_MAX = 1e-3\n\n# Weight Decay Ratio as Ratio of Learning Rate\nWD_RATIO = 0.05\n\n# Length of Phrase + EOS Token\nMAX_PHRASE_LENGTH = 31 + 1\n\n# Whether to Train The model\nTRAIN_MODEL = True\n\n# Whether to Load Pretrained Weights\nLOAD_WEIGHTS = False\n\n# Learning Rate Warmup Method [log,exp]\nWARMUP_METHOD = 'exp'","metadata":{"papermill":{"duration":0.034637,"end_time":"2023-07-02T11:33:08.963209","exception":false,"start_time":"2023-07-02T11:33:08.928572","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:34:27.927307Z","iopub.execute_input":"2023-09-24T16:34:27.927637Z","iopub.status.idle":"2023-09-24T16:34:27.935772Z","shell.execute_reply.started":"2023-09-24T16:34:27.927607Z","shell.execute_reply":"2023-09-24T16:34:27.934542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading the Training DataFrame\n\n#### Debug Mode Data Sampling\n- The code checks the value of the `DEBUG` flag. If it is set to `True`, the code reads a subset of the training data by loading the first 5000 rows from the CSV file located at `/kaggle/input/asl-fingerspelling/train.csv`. This subset is useful for debugging and faster execution. If `DEBUG` is set to `False`, the code reads the entire training dataset.\n\n#### Reading the CSV File\n- The training dataset is read from the CSV file located at `/kaggle/input/asl-fingerspelling/train.csv`. This file presumably contains information about the training examples, such as sequence IDs, file paths, phrases, and other relevant data.\n\n#### Creating a Sequence ID Index\n- The code constructs an index called `train_sequence_id` using the `sequence_id` column from the training dataset. This index can be used to quickly retrieve data associated with specific sequence IDs.\n\n#### File Path Construction\n- A function named `get_file_path` is defined to construct the complete file path from a given path. This function is applied to the `path` column of the training dataset using the `apply` method. The result is stored in a new column called `file_path`. This step is likely done to make it easier to access the actual data files associated with each training example.\n\n#### Displaying the First Few Rows\n- The `head()` method is used to display the first few rows of the training dataset with the newly added `file_path` column.\n\nThis code prepares the training dataset by either loading a subset (in debug mode) or the full dataset (in non-debug mode) and constructs additional columns for file paths. The dataset is now ready for further processing and model training.\nSince `DEBUG=False`, we load the entire training dataset. We add a new column file_path, containing the complete `file_path` for the parquet files.","metadata":{"papermill":{"duration":0.023106,"end_time":"2023-07-02T11:33:09.114637","exception":false,"start_time":"2023-07-02T11:33:09.091531","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Read Train DataFrame\nif DEBUG:\n    train = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv').head(5000)\nelse:\n    train = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\n\n# this will be used to construct TFLite model\ntrain_sequence_id = train.set_index('sequence_id')\n\n# Get complete file path to file\ndef get_file_path(path):\n    return f'/kaggle/input/asl-fingerspelling/{path}'\n\ntrain['file_path'] = train['path'].apply(get_file_path)\n\ntrain.head()","metadata":{"papermill":{"duration":0.236261,"end_time":"2023-07-02T11:33:09.374134","exception":false,"start_time":"2023-07-02T11:33:09.137873","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:34:27.937423Z","iopub.execute_input":"2023-09-24T16:34:27.937877Z","iopub.status.idle":"2023-09-24T16:34:28.072823Z","shell.execute_reply.started":"2023-09-24T16:34:27.937847Z","shell.execute_reply":"2023-09-24T16:34:28.071772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Example Parquet\n\nLet's remember the format of a Parquet file that we have saved before preprocessing:","metadata":{}},{"cell_type":"code","source":"# Unique Parquet Files\nINFERENCE_FILE_PATHS = pd.Series(\n        glob.glob('/kaggle/input//aslfr-eda-preprocessing-dataset-for-beginners/train_landmark_subsets/*')\n    )\n\n# Read a Parquet File\nexample_parquet_df = pd.read_parquet(INFERENCE_FILE_PATHS[0])\n\n# Each parquet file contains 1000 recordings\nprint(f'Number of Unique Recording: {example_parquet_df.index.nunique()}')\n# Display DataFrame layout\ndisplay(example_parquet_df.head())","metadata":{"papermill":{"duration":1.512276,"end_time":"2023-07-02T11:33:49.075196","exception":false,"start_time":"2023-07-02T11:33:47.56292","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:34:28.074434Z","iopub.execute_input":"2023-09-24T16:34:28.074782Z","iopub.status.idle":"2023-09-24T16:34:29.681282Z","shell.execute_reply.started":"2023-09-24T16:34:28.074748Z","shell.execute_reply":"2023-09-24T16:34:29.680335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization using MediaPipe","metadata":{}},{"cell_type":"markdown","source":"## Creating Animation from Images\n\n### Configuration and Initialization\n\nThe following Python code defines a function `create_animation` that is responsible for creating an animation from a list of images. Before defining the function, it performs some necessary configurations.This function sets up a figure and axis, initializes an image, defines an animation update function, and returns an animation that cycles through the input images, creating an animated sequence.\n\n```python\n# Configuration settings for animation embedding\nmatplotlib.rcParams['animation.embed_limit'] = 2**128\nmatplotlib.rcParams['savefig.pad_inches'] = 0\nrc('animation', html='jshtml')\n```\n\n### Function Definition: `create_animation`\n\nThe `create_animation` function takes a list of images as input and returns an animation.\n\n```python\ndef create_animation(images):\n```\n\n### Figure and Axis Initialization\n\nInside the function, a figure (`fig`) and an axis (`ax`) are created using Matplotlib to set up the canvas for the animation. The figure is defined with dimensions 6x9 inches, and the axis covers the entire figure area.\n\n```python\n    fig = plt.figure(figsize=(6, 9))\n    ax = plt.Axes(fig, [0., 0., 1., 1.])\n```\n\n### Configuring the Axis\n\nThe axis's visibility is turned off, and the configured axis is added to the figure.\n\n```python\n    ax.set_axis_off()\n    fig.add_axes(ax)\n```\n\n### Image Initialization\n\nAn initial image (`im`) is created from the first image in the input list (`images[0]`). This image is initially displayed using a grayscale colormap.\n\n```python\n    im = ax.imshow(images[0], cmap=\"gray\")\n```\n\n### Animation Function Definition\n\nA function `animate_func` is defined to update the displayed image in the animation. It takes an index `i` as input, sets the image's array to the `i`-th image in the list, and returns a list containing the updated image.\n\n```python\n    def animate_func(i):\n        im.set_array(images[i])\n        return [im]\n```\n\n### Creating the Animation\n\nFinally, the code uses Matplotlib's `animation.FuncAnimation` to create the animation. It associates the animation with the defined figure, specifies the `animate_func` as the update function, sets the number of frames to be equal to the number of images in the list, and specifies the interval between frames to control the animation speed.\n\n```python\n    return animation.FuncAnimation(fig, animate_func, frames=len(images), interval=1000/10)\n```\n","metadata":{}},{"cell_type":"code","source":"# Function create animation from images.\n\nmatplotlib.rcParams['animation.embed_limit'] = 2**128\nmatplotlib.rcParams['savefig.pad_inches'] = 0\nrc('animation', html='jshtml')\n\ndef create_animation(images):\n    fig = plt.figure(figsize=(6, 9))\n    ax = plt.Axes(fig, [0., 0., 1., 1.])\n    ax.set_axis_off()\n    fig.add_axes(ax)\n    im=ax.imshow(images[0], cmap=\"gray\")\n    plt.close(fig)\n    \n    def animate_func(i):\n        im.set_array(images[i])\n        return [im]\n\n    return animation.FuncAnimation(fig, animate_func, frames=len(images), interval=1000/10)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:34:29.682747Z","iopub.execute_input":"2023-09-24T16:34:29.683807Z","iopub.status.idle":"2023-09-24T16:34:29.692357Z","shell.execute_reply.started":"2023-09-24T16:34:29.683770Z","shell.execute_reply":"2023-09-24T16:34:29.691191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hand Landmark Extraction\n\n### Importing Mediapipe Libraries\n\nThe following Python code imports necessary modules from the Mediapipe library.\n\n```python\nmp_pose = mediapipe.solutions.pose\nmp_hands = mediapipe.solutions.hands\nmp_face_mesh = mediapipe.solutions.face_mesh\nmp_drawing = mediapipe.solutions.drawing_utils \nmp_drawing_styles = mediapipe.solutions.drawing_styles\n```\n\n### Function Definition: `get_hands`\n\nThe code defines a Python function named `get_hands`. This function is responsible for extracting hand landmark data from a DataFrame and converting it into images using the Mediapipe library.\n\n```python\ndef get_hands(seq_df):\n```\n\n### Initialization\n\nInside the `get_hands` function, two lists are initialized to store the generated images and detected hand landmarks.\n\n```python\n    images = []\n    all_hand_landmarks = []\n```\n\n### Loop Over DataFrame Rows\n\nThe code iterates over the rows of a DataFrame named `seq_df`, which is expected to contain hand landmark data.\n\n### Extracting Hand Landmark Data\n\nWithin the loop, the code extracts hand landmark data (x, y, and z coordinates) for both the right and left hands from the DataFrame.\n\n### Creating Blank Images\n\nTwo blank images, `right_hand_image` and `left_hand_image`, of size 600x600 pixels are created to visualize the hand landmarks.\n\n### Creating Landmark Lists\n\nNormalized landmark lists for the right and left hands, named `right_hand_landmarks` and `left_hand_landmarks`, are initialized. These objects will store the normalized landmark data.\n\n### Drawing Hand Landmarks\n\nThe code uses the `mp_drawing.draw_landmarks` function to draw the hand landmarks on the respective images (`right_hand_image` and `left_hand_image`). This function takes the image, landmark data, hand connections, and drawing styles as arguments.\n\n### Appending Images and Landmarks\n\nThe generated images and detected hand landmarks (as `NormalizedLandmarkList` objects) for both hands are appended to the `images` and `all_hand_landmarks` lists, respectively.\n\n### Returning Results\n\nThe `get_hands` function returns a list of images and a list of hand landmarks for each frame in the input DataFrame.\n\n```python\n    return images, all_hand_landmarks\n```\n\n\n```\n","metadata":{}},{"cell_type":"code","source":"# Extract the landmark data and convert it to an image using medipipe library.\n# This function extracts the data for both hands.\n\nmp_pose = mediapipe.solutions.pose\nmp_hands = mediapipe.solutions.hands\nmp_face_mesh = mediapipe.solutions.face_mesh\nmp_drawing = mediapipe.solutions.drawing_utils \nmp_drawing_styles = mediapipe.solutions.drawing_styles\n\ndef get_hands(seq_df):\n    images = []\n    all_hand_landmarks = []\n    for seq_idx in range(len(seq_df)):\n        x_hand = seq_df.iloc[seq_idx].filter(regex=\"x_right_hand.*\").values\n        y_hand = seq_df.iloc[seq_idx].filter(regex=\"y_right_hand.*\").values\n        z_hand = seq_df.iloc[seq_idx].filter(regex=\"z_right_hand.*\").values\n\n        right_hand_image = np.zeros((600, 600, 3))\n\n        right_hand_landmarks = landmark_pb2.NormalizedLandmarkList()\n        \n        for x, y, z in zip(x_hand, y_hand, z_hand):\n            right_hand_landmarks.landmark.add(x=x, y=y, z=z)\n\n        mp_drawing.draw_landmarks(\n                right_hand_image,\n                right_hand_landmarks,\n                mp_hands.HAND_CONNECTIONS,\n                landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style())\n        \n        x_hand = seq_df.iloc[seq_idx].filter(regex=\"x_left_hand.*\").values\n        y_hand = seq_df.iloc[seq_idx].filter(regex=\"y_left_hand.*\").values\n        z_hand = seq_df.iloc[seq_idx].filter(regex=\"z_left_hand.*\").values\n        \n        left_hand_image = np.zeros((600, 600, 3))\n        \n        left_hand_landmarks = landmark_pb2.NormalizedLandmarkList()\n        for x, y, z in zip(x_hand, y_hand, z_hand):\n            left_hand_landmarks.landmark.add(x=x, y=y, z=z)\n\n        mp_drawing.draw_landmarks(\n                left_hand_image,\n                left_hand_landmarks,\n                mp_hands.HAND_CONNECTIONS,\n                landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style())\n        \n        images.append([right_hand_image.astype(np.uint8), left_hand_image.astype(np.uint8)])\n        all_hand_landmarks.append([right_hand_landmarks, left_hand_landmarks])\n    return images, all_hand_landmarks","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:34:29.697485Z","iopub.execute_input":"2023-09-24T16:34:29.697750Z","iopub.status.idle":"2023-09-24T16:34:29.712698Z","shell.execute_reply.started":"2023-09-24T16:34:29.697726Z","shell.execute_reply":"2023-09-24T16:34:29.711763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extracting Face Landmark Data and Converting to Images\n\n### Function Parameters\n\nThe function `get_face` takes a DataFrame `seq_df` as input. This DataFrame is assumed to contain the facial landmark data. This function processes facial landmark data, creates annotated images with facial landmarks, and stores both images and landmark data in lists for further analysis or visualization.\n\n```python\ndef get_face(seq_df):\n```\n\n### Image Storage Initialization\n\nInside the function, two empty lists (`images` and `all_face_landmarks`) are created to store images and face landmark data, respectively. These lists will be populated as the function processes each sequence.\n\n```python\n    images = []\n    all_face_landmarks = []\n```\n\n### Loop Through Sequences\n\nThe function iterates through each sequence in the input DataFrame `seq_df`.\n\n```python\n    for seq_idx in range(len(seq_df)):\n```\n\n### Extracting Facial Landmark Data\n\nThe x, y, and z coordinates of facial landmarks are extracted from the DataFrame using regular expressions to filter columns with names like \"x_face.*\", \"y_face.*\", and \"z_face.*\".\n\n```python\n        x_face = seq_df.iloc[seq_idx].filter(regex=\"x_face.*\").values\n        y_face = seq_df.iloc[seq_idx].filter(regex=\"y_face.*\").values\n        z_face = seq_df.iloc[seq_idx].filter(regex=\"z_face.*\").values\n```\n\n### Initializing an Annotated Image\n\nAn annotated image is initialized as a black canvas with dimensions 900x600 pixels to draw the facial landmarks on.\n\n```python\n        annotated_image = np.zeros((900, 600, 3))\n```\n\n### Creating a `NormalizedLandmarkList`\n\nA `NormalizedLandmarkList` object, `face_landmarks`, is created to store the facial landmark data.\n\n```python\n        face_landmarks = landmark_pb2.NormalizedLandmarkList()\n```\n\n### Drawing Facial Landmarks\n\nMediapipe's `mp_drawing.draw_landmarks` function is used to draw the facial landmarks on the annotated image. Two types of facial landmarks are drawn: mesh tessellation and contour landmarks.\n\n```python\n        mp_drawing.draw_landmarks(\n          image=annotated_image,\n          landmark_list=face_landmarks,\n          connections=mp_face_mesh.FACEMESH_TESSELATION,\n          landmark_drawing_spec=None,\n          connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n          \n        mp_drawing.draw_landmarks(\n          image=annotated_image,\n          landmark_list=face_landmarks,\n          connections=mp_face_mesh.FACEMESH_CONTOURS,\n          landmark_drawing_spec=None,\n          connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n```\n\n### Appending Data to Lists\n\nThe annotated image, represented as a NumPy array, is appended to the `images` list. The `face_landmarks` object is appended to the `all_face_landmarks` list.\n\n```python\n        images.append(annotated_image.astype(np.uint8))\n        all_face_landmarks.append(face_landmarks)\n```\n\n### Return Values\n\nThe function returns two lists: `images`, which contains the annotated face images, and `all_face_landmarks`, which contains the facial landmark data.\n\n```python\n    return images, all_face_landmarks\n```\n\n\n```\n","metadata":{}},{"cell_type":"code","source":"# Extract the landmark data and convert it to an image using medipipe library.\n# This function extracts the data for face\n\ndef get_face(seq_df):\n    images = []\n    all_face_landmarks = []\n    for seq_idx in range(len(seq_df)):\n        x_face = seq_df.iloc[seq_idx].filter(regex=\"x_face.*\").values\n        y_face = seq_df.iloc[seq_idx].filter(regex=\"y_face.*\").values\n        z_face = seq_df.iloc[seq_idx].filter(regex=\"z_face.*\").values\n\n        annotated_image = np.zeros((900, 600, 3))\n\n        face_landmarks = landmark_pb2.NormalizedLandmarkList()\n        for x, y, z in zip(x_face, y_face, z_face):\n            face_landmarks.landmark.add(x=x, y=y, z=z)\n\n        mp_drawing.draw_landmarks(\n          image=annotated_image,\n          landmark_list=face_landmarks,\n          connections=mp_face_mesh.FACEMESH_TESSELATION,\n          landmark_drawing_spec=None,\n          connection_drawing_spec=mp_drawing_styles\n          .get_default_face_mesh_tesselation_style())\n        mp_drawing.draw_landmarks(\n          image=annotated_image,\n          landmark_list=face_landmarks,\n          connections=mp_face_mesh.FACEMESH_CONTOURS,\n          landmark_drawing_spec=None,\n          connection_drawing_spec=mp_drawing_styles\n          .get_default_face_mesh_contours_style())\n\n        images.append(annotated_image.astype(np.uint8))\n        all_face_landmarks.append(face_landmarks)\n    return images, all_face_landmarks\n\nsequence_id, file_id, phrase = dataset_df.iloc[random.randint(0 , dataset_df.shape[0])][['sequence_id', 'file_id', 'phrase']]\nsample_sequence_df = pq.read_table(f\"/kaggle/input/asl-fingerspelling/train_landmarks/{str(file_id)}.parquet\",\n    filters=[[('sequence_id', '=', sequence_id)],]).to_pandas()\nface_images, face_landmarks = get_face(sample_sequence_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:34:29.713966Z","iopub.execute_input":"2023-09-24T16:34:29.714303Z","iopub.status.idle":"2023-09-24T16:34:40.471525Z","shell.execute_reply.started":"2023-09-24T16:34:29.714273Z","shell.execute_reply":"2023-09-24T16:34:40.469964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extracting Pose Landmark Data and Converting to Images\n\n### Function Definition: `get_pose`\n\nThe function `get_pose` processes pose landmark data stored in a DataFrame (`seq_df`) and converts it into images. The code relies on the Mediapipe library for this task.\n\n1. **Data Extraction**: The code iterates through sequences in `seq_df` and extracts x, y, and z coordinates of pose landmarks using regular expressions.\n\n2. **Image Initialization**: An empty image (`annotated_image`) is initialized as a black canvas with dimensions 900x600 pixels.\n\n3. **Data Processing**: The x, y, and z coordinates are collected into `data_points`, which are stored as NumPy arrays.\n\n4. **Landmark List Creation**: A `NormalizedLandmarkList` object (`pose_landmarks`) is created to store the pose landmark data.\n\n5. **Adding Landmarks**: The code adds the pose landmarks to `pose_landmarks` based on the data points.\n\n6. **Drawing Landmarks**: Pose landmarks are drawn on the `annotated_image` using Mediapipe's `mp_drawing.draw_landmarks` function.\n\n7. **Data Storage**: The annotated image (converted to a NumPy array) and the `pose_landmarks` object are appended to the `images` and `all_pose_landmarks` lists, respectively.\n\n### Return Values\n\nThe function returns two lists: `images` containing annotated pose images and `all_pose_landmarks` containing pose landmark data.\n\n`get_pose` processes pose landmark data, creates annotated pose images, and stores them along with landmark data for further use or visualization.\n","metadata":{}},{"cell_type":"code","source":"def get_pose(seq_df):\n    images = []\n    all_pose_landmarks = []\n    for seq_idx in range(len(seq_df)):\n        x_pose = seq_df.iloc[seq_idx].filter(regex=\"x_pose.*\").values\n        y_pose = seq_df.iloc[seq_idx].filter(regex=\"y_pose.*\").values\n        z_pose = seq_df.iloc[seq_idx].filter(regex=\"z_pose.*\").values\n\n        annotated_image = np.zeros((900, 600, 3))\n        \n        data_points = []\n        for x, y, z in zip(x_pose, y_pose, z_pose):\n            data_points.append(np.array([x, y, z]))\n\n        pose_landmarks = landmark_pb2.NormalizedLandmarkList()\n        for row in data_points:\n            pose_landmarks.landmark.add(x=row[0], y=row[1], z=row[2])\n\n        mp_drawing.draw_landmarks(\n                annotated_image,\n                pose_landmarks,\n                mp_pose.POSE_CONNECTIONS,\n                landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n        images.append(annotated_image.astype(np.uint8))\n        all_pose_landmarks.append(pose_landmarks)\n    return images, all_pose_landmarks\n\npose_images, pose_landmarks = get_pose(sample_sequence_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:34:40.472743Z","iopub.execute_input":"2023-09-24T16:34:40.473339Z","iopub.status.idle":"2023-09-24T16:34:43.179779Z","shell.execute_reply.started":"2023-09-24T16:34:40.473295Z","shell.execute_reply":"2023-09-24T16:34:43.178342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Converting Landmark Data to Images and Normalizing\n\n### Function Definition: `get_all_images`\n\n`get_all_images` function processes landmark data, normalizes it, and converts it into images using the Mediapipe library. The function handles pose, hand, and face landmarks separately.\n\n1. **Data Extraction**: The code retrieves pose, hand, and face landmarks for each sequence from the input DataFrame (`seq_df`).\n\n2. **Normalization and Scaling**: The landmarks are normalized and scaled to fit within a [0, 1] range. Scaling is done based on the maximum x and y values among all landmarks. Additionally, the landmarks are shifted to center the image around the shoulders and hips.\n\n3. **Image Creation**: Images are initialized as black canvases with dimensions 900x600 pixels.\n\n4. **Drawing Landmarks**: Mediapipe functions are used to draw the normalized landmarks on the images for pose, right hand, left hand, and face.\n\n5. **Data Storage**: The resulting images (as NumPy arrays), landmark data (as NumPy arrays), and landmark objects are appended to various lists.\n\n6. **Return Values**: The function returns three lists: `all_images` containing annotated images, `all_landmarks_data` containing normalized landmark data, and `all_landmarks` containing landmark objects.\n\n`get_all_images` processes and normalizes landmark data for multiple body parts and generates corresponding annotated images for visualization or further analysis.\n","metadata":{}},{"cell_type":"code","source":"def convert_landmark_to_npy(landmarklist):\n    return np.array([np.array([landmark.x, landmark.y, landmark.z]) for landmark in landmarklist.landmark])\n\ndef get_all_images(seq_df):\n    pose_images, pose_landmarks = get_pose(seq_df)\n    hand_images, hand_landmarks = get_hands(seq_df)\n    face_images, face_landmarks = get_face(seq_df)\n    \n    all_images = []\n    all_landmarks_data = []\n    all_landmarks = []\n    for seq_idx in tqdm(range(len(pose_landmarks))):\n        pose_landmark_np = convert_landmark_to_npy(pose_landmarks[seq_idx])\n        right_hand_landmark_np = convert_landmark_to_npy(hand_landmarks[seq_idx][0])\n        left_hand_landmark_np = convert_landmark_to_npy(hand_landmarks[seq_idx][1])\n        face_landmark_np = convert_landmark_to_npy(face_landmarks[seq_idx])\n        \n        # Pool all landmarks together to find min and max coordinates\n        pooled_landmarks = np.vstack((pose_landmark_np, right_hand_landmark_np, left_hand_landmark_np, face_landmark_np))\n        pooled_min = np.nanmin(pooled_landmarks, axis=0)\n        pooled_max = np.nanmax(pooled_landmarks, axis=0)\n        \n        # Use the max of x and y scaling to proportionally scale the image. We don't need to scale z for 2D image\n        scaling_factor = np.nanmax(pooled_max[:2])\n        pooled_scaled_min = np.nanmin(pooled_landmarks / scaling_factor, axis=0)\n\n        pose_landmark_np_normed = (pose_landmark_np / scaling_factor) - pooled_scaled_min\n        \n        # Center the image around shoulder and hips. Makes for a better visualization\n        x_shift = ((1-(pose_landmark_np_normed[23]+pose_landmark_np_normed[24]))/2)[0]\n        axis_shift = np.array([x_shift, 0, 0])\n        \n        pose_landmark_np_normed = pose_landmark_np_normed + axis_shift\n        right_hand_landmark_np_normed = (right_hand_landmark_np / scaling_factor) - pooled_scaled_min + axis_shift\n        left_hand_landmark_np_normed = (left_hand_landmark_np / scaling_factor) - pooled_scaled_min  + axis_shift\n        face_landmark_np_normed = (face_landmark_np / scaling_factor) - pooled_scaled_min + axis_shift\n        \n        # Now that we have scaled and shifted the landmarks to fit into a [0, 1] range, we can start plotting them using mediapipe APIs\n        # BG image with zeros\n        image = np.zeros((900, 600, 3))\n        \n        # Pose\n        pose_landmark_np_normed_z = landmark_pb2.LandmarkList()\n        for row in pose_landmark_np_normed:\n            pose_landmark_np_normed_z.landmark.add(x=row[0], y=row[1], z=row[2])\n\n        mp_drawing.draw_landmarks(\n                    image,\n                    pose_landmark_np_normed_z,\n                    mp_pose.POSE_CONNECTIONS,\n                    landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n\n        # Right hand\n        right_hand_landmark_np_normed_z = landmark_pb2.LandmarkList()\n        for row in right_hand_landmark_np_normed:\n            right_hand_landmark_np_normed_z.landmark.add(x=row[0], y=row[1], z=row[2])\n\n        mp_drawing.draw_landmarks(\n                    image,\n                    right_hand_landmark_np_normed_z,\n                    mp_hands.HAND_CONNECTIONS,\n                    landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style())\n        \n        # Left hand\n        left_hand_landmark_np_normed_z = landmark_pb2.LandmarkList()\n        for row in left_hand_landmark_np_normed:\n            left_hand_landmark_np_normed_z.landmark.add(x=row[0], y=row[1], z=row[2])\n\n        mp_drawing.draw_landmarks(\n                    image,\n                    left_hand_landmark_np_normed_z,\n                    mp_hands.HAND_CONNECTIONS,\n                    landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style())\n        \n        # Face\n        face_landmark_np_normed_z = landmark_pb2.LandmarkList()\n        for row in face_landmark_np_normed:\n            face_landmark_np_normed_z.landmark.add(x=row[0], y=row[1], z=row[2])\n\n        mp_drawing.draw_landmarks(\n            image=image,\n            landmark_list=face_landmark_np_normed_z,\n            connections=mp_face_mesh.FACEMESH_TESSELATION,\n            landmark_drawing_spec=None,\n            connection_drawing_spec=mp_drawing_styles\n            .get_default_face_mesh_tesselation_style())\n        \n        mp_drawing.draw_landmarks(\n            image=image,\n            landmark_list=face_landmark_np_normed_z,\n            connections=mp_face_mesh.FACEMESH_CONTOURS,\n            landmark_drawing_spec=None,\n            connection_drawing_spec=mp_drawing_styles\n            .get_default_face_mesh_contours_style())\n        \n        # Iris data not available. So ignoring the iris visualization.\n        \n        all_images.append(image.astype(np.uint8))\n        all_landmarks_data.append([pose_landmark_np_normed, right_hand_landmark_np_normed_z, left_hand_landmark_np_normed_z, face_landmark_np_normed_z])\n        all_landmarks.append([pose_landmark_np_normed_z, right_hand_landmark_np_normed_z, left_hand_landmark_np_normed_z, face_landmark_np_normed_z])\n    return all_images, all_landmarks_data, all_landmarks","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:34:43.181235Z","iopub.execute_input":"2023-09-24T16:34:43.181868Z","iopub.status.idle":"2023-09-24T16:34:43.207729Z","shell.execute_reply.started":"2023-09-24T16:34:43.181832Z","shell.execute_reply":"2023-09-24T16:34:43.206741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating Images and Landmark Data\n\n### Image and Landmark Data Generation\n\nTwo sets of images and associated landmark data are generated using the `get_all_images` function:\n\n1. **`all_images, all_landmarks_data, all_landmarks`**:\n   - This line of code calls the `get_all_images` function on the `sample_sequence_df` DataFrame. It generates images (`all_images`) and associated landmark data (`all_landmarks_data`) for the provided sequence data (`sample_sequence_df`). The `all_landmarks` list contains landmark objects.\n   \n2. **`all_images1, _, _`**:\n   - A second set of images (`all_images1`) is generated using the same `get_all_images` function. However, this time, the `sample_sequence_df` DataFrame is filled using the forward-fill method (`fillna(method='ffill')`) to handle missing values.\n\nThese generated images and landmark data can be used for various purposes, such as visualization, analysis, or as input data for machine learning models. The filled DataFrame with forward-fill can be useful for scenarios where continuity of data is important.\n","metadata":{}},{"cell_type":"code","source":"all_images, all_landmarks_data, all_landmarks = get_all_images(sample_sequence_df)\nall_images1, _, _ = get_all_images(sample_sequence_df.fillna(method='ffill'))","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:34:43.212087Z","iopub.execute_input":"2023-09-24T16:34:43.214470Z","iopub.status.idle":"2023-09-24T16:35:17.236366Z","shell.execute_reply.started":"2023-09-24T16:34:43.214436Z","shell.execute_reply":"2023-09-24T16:35:17.235218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The create_animation(all_images1) function generates an animation from a list of images (all_images1) for visualization. It configures settings, creates a figure, displays images frame by frame, and returns the animation object.","metadata":{}},{"cell_type":"code","source":"create_animation(all_images1)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:35:17.237797Z","iopub.execute_input":"2023-09-24T16:35:17.238616Z","iopub.status.idle":"2023-09-24T16:35:37.431956Z","shell.execute_reply.started":"2023-09-24T16:35:17.238580Z","shell.execute_reply":"2023-09-24T16:35:37.428282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Processing","metadata":{}},{"cell_type":"markdown","source":"## Load X/y\n\n### Loading Training and Validation Data\n\n#### Using Validation Data (USE_VAL=True)\n- If `USE_VAL` is set to `True`, the code loads both training and validation datasets.\n\n#### Training Data\n- `X_train` is loaded from the file `/kaggle/input/aslfr-eda-preprocessing-dataset-for-beginners/X_train.npy`.\n- `y_train` is loaded from the file `/kaggle/input/aslfr-eda-preprocessing-dataset-for-beginners/y_train.npy` and limited to a maximum phrase length of `MAX_PHRASE_LENGTH`.\n- The variable `N_TRAIN_SAMPLES` is assigned the length of `X_train`.\n\n#### Validation Data\n- `X_val` is loaded from the file `/kaggle/input/aslfr-eda-preprocessing-dataset-for-beginners/X_val.npy`.\n- `y_val` is loaded from the file `/kaggle/input/aslfr-eda-preprocessing-dataset-for-beginners/y_val.npy` and limited to a maximum phrase length of `MAX_PHRASE_LENGTH`.\n- The variable `N_VAL_SAMPLES` is assigned the length of `X_val`.\n\n#### Displaying Shapes\n- The shapes of `X_train`, `X_val`, `y_train`, and `y_val` are displayed to provide information about the dimensions of the loaded data.\n\n#### Using All Data (USE_VAL=False)\n- If `USE_VAL` is set to `False`, the code loads only the training dataset.\n\n#### Training Data\n- `X_train` is loaded from the file `/kaggle/input/aslfr-eda-preprocessing-dataset-for-beginners/X.npy`.\n- `y_train` is loaded from the file `/kaggle/input/aslfr-eda-preprocessing-dataset-for-beginners/y.npy` and limited to a maximum phrase length of `MAX_PHRASE_LENGTH`.\n- The variable `N_TRAIN_SAMPLES` is assigned the length of `X_train`.\n\n#### Displaying Shapes\n- The shapes of `X_train` and `y_train` are displayed to provide information about the dimensions of the loaded data.\n\nWe load and prepare the training and validation datasets, depending on whether we want to use validation during training or not.\n","metadata":{"papermill":{"duration":0.023663,"end_time":"2023-07-02T11:33:09.662914","exception":false,"start_time":"2023-07-02T11:33:09.639251","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Train/Validation\nif USE_VAL:\n    # TRAIN\n    X_train = np.load('/kaggle/input/aslfr-eda-preprocessing-dataset-for-beginners/X_train.npy')\n    y_train = np.load('/kaggle/input/aslfr-eda-preprocessing-dataset-for-beginners/y_train.npy')[:,:MAX_PHRASE_LENGTH]\n    N_TRAIN_SAMPLES = len(X_train)\n    # VAL\n    X_val = np.load('/kaggle/input/aslfr-eda-preprocessing-dataset-for-beginners/X_val.npy')\n    y_val = np.load('/kaggle/input/aslfr-eda-preprocessing-dataset-for-beginners/y_val.npy')[:,:MAX_PHRASE_LENGTH]\n    N_VAL_SAMPLES = len(X_val)\n    # Shapes\n    print(f'X_train shape: {X_train.shape}, X_val shape: {X_val.shape}')\n    print(f'y_train shape: {X_train.shape}, y_val shape: {X_val.shape}')\n    \n# Train On All Data\nelse:\n    # TRAIN\n    X_train = np.load('/kaggle/input/aslfr-eda-preprocessing-dataset-for-beginners/X.npy')\n    y_train = np.load('/kaggle/input/aslfr-eda-preprocessing-dataset-for-beginners/y.npy')[:,:MAX_PHRASE_LENGTH]\n    N_TRAIN_SAMPLES = len(X_train)\n    print(f'X_train shape: {X_train.shape}')\n    print(f'y_train shape: {y_train.shape}')","metadata":{"papermill":{"duration":37.639487,"end_time":"2023-07-02T11:33:47.326164","exception":false,"start_time":"2023-07-02T11:33:09.686677","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:35:37.433082Z","iopub.execute_input":"2023-09-24T16:35:37.434023Z","iopub.status.idle":"2023-09-24T16:36:13.823939Z","shell.execute_reply.started":"2023-09-24T16:35:37.433986Z","shell.execute_reply":"2023-09-24T16:36:13.822958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Example Batch\n\n### Creating Example Batches for Debugging\n\n#### Large Example Batch\n- `N_EXAMPLE_BATCH_SAMPLES` is set to 1024, determining the number of samples in the large example batch.\n- The variable `X_batch` is a dictionary with two keys: `'frames'` and `'phrase'`.\n  - `'frames'` contains a copy of the first `N_EXAMPLE_BATCH_SAMPLES` samples from `X_train`.\n  - `'phrase'` contains a copy of the first `N_EXAMPLE_BATCH_SAMPLES` samples from `y_train`.\n- The variable `y_batch` contains a copy of the first `N_EXAMPLE_BATCH_SAMPLES` samples from `y_train`.\n\n#### Small Example Batch\n- `N_EXAMPLE_BATCH_SAMPLES_SMALL` is set to 32, determining the number of samples in the small example batch.\n- The variable `X_batch_small` is a dictionary with two keys: `'frames'` and `'phrase'`.\n  - `'frames'` contains a copy of the first `N_EXAMPLE_BATCH_SAMPLES_SMALL` samples from `X_train`.\n  - `'phrase'` contains a copy of the first `N_EXAMPLE_BATCH_SAMPLES_SMALL` samples from `y_train`.\n- The variable `y_batch_small` contains a copy of the first `N_EXAMPLE_BATCH_SAMPLES_SMALL` samples from `y_train`.\n\nThese example batches are useful for debugging and testing the machine learning model during development.\n","metadata":{"papermill":{"duration":0.02373,"end_time":"2023-07-02T11:33:47.374154","exception":false,"start_time":"2023-07-02T11:33:47.350424","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Example Batch For Debugging\nN_EXAMPLE_BATCH_SAMPLES = 1024\nN_EXAMPLE_BATCH_SAMPLES_SMALL = 32\n# Example Batch\nX_batch = {\n    'frames': np.copy(X_train[:N_EXAMPLE_BATCH_SAMPLES]),\n    'phrase': np.copy(y_train[:N_EXAMPLE_BATCH_SAMPLES]),\n#     'phrase_type': np.copy(y_phrase_type_train[:N_EXAMPLE_BATCH_SAMPLES]),\n}\ny_batch = np.copy(y_train[:N_EXAMPLE_BATCH_SAMPLES])\n# Small Example Batch\nX_batch_small = {\n    'frames': np.copy(X_train[:N_EXAMPLE_BATCH_SAMPLES_SMALL]),\n    'phrase': np.copy(y_train[:N_EXAMPLE_BATCH_SAMPLES_SMALL]),\n#     'phrase_type': np.copy(y_phrase_type_train[:N_EXAMPLE_BATCH_SAMPLES_SMALL]),\n}\ny_batch_small = np.copy(y_train[:N_EXAMPLE_BATCH_SAMPLES_SMALL])","metadata":{"papermill":{"duration":0.093503,"end_time":"2023-07-02T11:33:47.491607","exception":false,"start_time":"2023-07-02T11:33:47.398104","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:13.825248Z","iopub.execute_input":"2023-09-24T16:36:13.825631Z","iopub.status.idle":"2023-09-24T16:36:13.849392Z","shell.execute_reply.started":"2023-09-24T16:36:13.825598Z","shell.execute_reply":"2023-09-24T16:36:13.848516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Landmark Indices\n\nSimilarly to the preprocessing notebook, we define the `get_idxs` function to obtain the indices of the left hand, right hand, and lips.\n\n### Get Indices in Original DataFrame\n\n#### Function Parameters\n- `df`: This parameter represents the input DataFrame from which you want to extract column indices and names.\n- `words_pos`: A list of words that you want to search for in column names.\n- `words_neg`: An optional list of words that you want to exclude from column names.\n- `ret_names`: A boolean parameter that determines whether to return both column indices and names (`True`) or only column indices (`False`).\n- `idxs_pos`: An optional list of specific column indices to consider. If provided, the function will only search for words in these columns.\n\n#### Function Behavior\n- The function `get_idxs` iterates over the `words_pos` list and, for each word, searches for matching column names in the input DataFrame `df`.\n- It excludes columns with names like \"frame.\"\n- If the `idxs_pos` parameter is provided, it restricts the search to columns with indices specified in `idxs_pos`.\n- If the `words_neg` list is provided, it excludes columns that contain any of the words in `words_neg`.\n- The function returns either both the column indices and names (if `ret_names` is `True`) or only the column indices (if `ret_names` is `False`).\n\n#### Return Values\n- If `ret_names` is `True`, the function returns two NumPy arrays:\n  - `idxs`: An array of column indices that match the search criteria.\n  - `names`: An array of column names that match the search criteria.\n- If `ret_names` is `False`, the function returns a single NumPy array containing the column indices that match the search criteria.\n\nThis function can be useful for selecting specific columns from a DataFrame based on certain criteria defined by the `words_pos`, `words_neg`, and `idxs_pos` parameters.\n","metadata":{"papermill":{"duration":0.039462,"end_time":"2023-07-02T11:33:49.150383","exception":false,"start_time":"2023-07-02T11:33:49.110921","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Get indices in original dataframe\ndef get_idxs(df, words_pos, words_neg=[], ret_names=True, idxs_pos=None):\n    idxs = []\n    names = []\n    for w in words_pos:\n        for col_idx, col in enumerate(example_parquet_df.columns):\n            # Exclude Non Landmark Columns\n            if col in ['frame']:\n                continue\n                \n            col_idx = int(col.split('_')[-1])\n            # Check if column name contains all words\n            if (w in col) and (idxs_pos is None or col_idx in idxs_pos) and all([w not in col for w in words_neg]):\n                idxs.append(col_idx)\n                names.append(col)\n    # Convert to Numpy arrays\n    idxs = np.array(idxs)\n    names = np.array(names)\n    # Returns either both column indices and names\n    if ret_names:\n        return idxs, names\n    # Or only columns indices\n    else:\n        return idxs","metadata":{"papermill":{"duration":0.051023,"end_time":"2023-07-02T11:33:49.235748","exception":false,"start_time":"2023-07-02T11:33:49.184725","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:13.850650Z","iopub.execute_input":"2023-09-24T16:36:13.851066Z","iopub.status.idle":"2023-09-24T16:36:13.859191Z","shell.execute_reply.started":"2023-09-24T16:36:13.851022Z","shell.execute_reply":"2023-09-24T16:36:13.858156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lips Landmark Face Ids\n\n#### Lips Landmark Indices\n- `LIPS_LANDMARK_IDXS`: This NumPy array contains a list of landmark indices associated with the lips and facial features. These indices represent specific points on the face related to lip movement and expression.\n\n#### Landmark Indices for Left/Right Hand (X and Y Axes Only)\n- `LEFT_HAND_IDXS0` and `RIGHT_HAND_IDXS0`: These arrays store the landmark indices for the left and right hands, respectively.\n- `LEFT_HAND_NAMES0` and `RIGHT_HAND_NAMES0`: Corresponding arrays that store the names of the left and right hand landmarks.\n- These landmarks are filtered to include only the X and Y axes; the Z axis information is excluded.\n- These landmarks represent points in the hand regions.\n\n#### Lips Landmark Indices (X and Y Axes Only)\n- `LIPS_IDXS0` and `LIPS_NAMES0`: These arrays contain the landmark indices and names for lip and facial landmarks.\n- Similar to the hand landmarks, they include only the X and Y axes information.\n- These landmarks are associated with facial features and lip movements.\n\n#### Column Organization\n- `COLUMNS0`: This array consolidates all the landmark names for the left hand, right hand, and lips. It represents the names of columns in the dataset that correspond to these landmarks.\n- `N_COLS0`: The total number of columns in the dataset that correspond to these landmarks.\n\n#### Dimension Information\n- `N_DIMS0`: This variable represents the number of dimensions used for these landmarks. In this case, only the X and Y axes are considered (2 dimensions).\n\n#### Output\n- Provides information about the number of columns (`N_COLS0`) and the dimensions (`N_DIMS0`) used for these landmark data.\n","metadata":{}},{"cell_type":"code","source":"# Lips Landmark Face Ids\nLIPS_LANDMARK_IDXS = np.array([\n        61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n        291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n        78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n        95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n    ])\n\n# Landmark Indices for Left/Right hand without z axis in raw data\nLEFT_HAND_IDXS0, LEFT_HAND_NAMES0 = get_idxs(example_parquet_df, ['left_hand'], ['z'])\nRIGHT_HAND_IDXS0, RIGHT_HAND_NAMES0 = get_idxs(example_parquet_df, ['right_hand'], ['z'])\nLIPS_IDXS0, LIPS_NAMES0 = get_idxs(example_parquet_df, ['face'], ['z'], idxs_pos=LIPS_LANDMARK_IDXS)\nCOLUMNS0 = np.concatenate((LEFT_HAND_NAMES0, RIGHT_HAND_NAMES0, LIPS_NAMES0))\nN_COLS0 = len(COLUMNS0)\n# Only X/Y axes are used\nN_DIMS0 = 2\n\nprint(f'N_COLS0: {N_COLS0}')","metadata":{"papermill":{"duration":0.053701,"end_time":"2023-07-02T11:33:49.324986","exception":false,"start_time":"2023-07-02T11:33:49.271285","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:13.860888Z","iopub.execute_input":"2023-09-24T16:36:13.861223Z","iopub.status.idle":"2023-09-24T16:36:13.874884Z","shell.execute_reply.started":"2023-09-24T16:36:13.861191Z","shell.execute_reply":"2023-09-24T16:36:13.873930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Landmark Indices in Subset of DataFrame with Only Selected Columns\n\n#### Left Hand Landmark Indices\n- `LEFT_HAND_IDXS`: This array contains the landmark indices for the left hand based on the selected columns (`LEFT_HAND_NAMES0`). These indices correspond to specific points on the left hand.\n\n#### Right Hand Landmark Indices\n- `RIGHT_HAND_IDXS`: Similar to the left hand, this array contains the landmark indices for the right hand based on the selected columns (`RIGHT_HAND_NAMES0`).\n\n#### Lips Landmark Indices\n- `LIPS_IDXS`: This array stores the landmark indices for the lips and facial features based on the selected columns (`LIPS_NAMES0`).\n\n#### Hand Landmark Indices\n- `HAND_IDXS`: This array concatenates the landmark indices for both the left and right hands. It represents all hand-related landmarks.\n\n#### Total Number of Columns\n- `N_COLS`: This variable represents the total number of columns in the dataset that correspond to the selected landmarks. It's equal to `N_COLS0` from the previous code block.\n\n#### Dimension Information\n- `N_DIMS`: Similar to before, this variable represents the number of dimensions used for these landmarks. In this case, only the X and Y axes are considered (2 dimensions).\n\n#### Output\n- Provides information about the number of columns (`N_COLS`) and dimensions (`N_DIMS`) used for these selected landmark data subsets.\n","metadata":{}},{"cell_type":"code","source":"# Landmark Indices in subset of dataframe with only COLUMNS selected\nLEFT_HAND_IDXS = np.argwhere(np.isin(COLUMNS0, LEFT_HAND_NAMES0)).squeeze()\nRIGHT_HAND_IDXS = np.argwhere(np.isin(COLUMNS0, RIGHT_HAND_NAMES0)).squeeze()\nLIPS_IDXS = np.argwhere(np.isin(COLUMNS0, LIPS_NAMES0)).squeeze()\nHAND_IDXS = np.concatenate((LEFT_HAND_IDXS, RIGHT_HAND_IDXS), axis=0)\nN_COLS = N_COLS0\n# Only X/Y axes are used\nN_DIMS = 2\n\nprint(f'N_COLS: {N_COLS}')","metadata":{"papermill":{"duration":0.050026,"end_time":"2023-07-02T11:33:49.408684","exception":false,"start_time":"2023-07-02T11:33:49.358658","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:13.876436Z","iopub.execute_input":"2023-09-24T16:36:13.876797Z","iopub.status.idle":"2023-09-24T16:36:13.888702Z","shell.execute_reply.started":"2023-09-24T16:36:13.876766Z","shell.execute_reply":"2023-09-24T16:36:13.887706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Indices in Processed Data by Axes (Dominant Hand)\n\n#### X-Axis Landmark Indices\n- `HAND_X_IDXS`: This array contains the indices of landmarks related to the dominant hand in the X-axis. It is generated by filtering the `LEFT_HAND_NAMES0` array to select only the landmarks with 'x' in their names.\n\n#### Y-Axis Landmark Indices\n- `HAND_Y_IDXS`: Similar to the X-axis, this array contains the indices of landmarks related to the dominant hand in the Y-axis. It is generated by filtering the `LEFT_HAND_NAMES0` array to select only the landmarks with 'y' in their names.\n\n#### Names in Processed Data by Axes\n- `HAND_X_NAMES`: This array stores the names of landmarks related to the dominant hand in the X-axis. It corresponds to the names of landmarks selected based on the `HAND_X_IDXS` indices.\n\n- `HAND_Y_NAMES`: Similar to `HAND_X_NAMES`, this array contains the names of landmarks related to the dominant hand in the Y-axis. It corresponds to the names of landmarks selected based on the `HAND_Y_IDXS` indices.\n\n#### Output\n- The code provides information about the selected landmark indices and their corresponding names for the X and Y axes of the dominant hand. These subsets of data can be used for further processing or analysis.\n","metadata":{}},{"cell_type":"code","source":"# Indices in processed data by axes with only dominant hand\nHAND_X_IDXS = np.array(\n        [idx for idx, name in enumerate(LEFT_HAND_NAMES0) if 'x' in name]\n    ).squeeze()\nHAND_Y_IDXS = np.array(\n        [idx for idx, name in enumerate(LEFT_HAND_NAMES0) if 'y' in name]\n    ).squeeze()\n# Names in processed data by axes\nHAND_X_NAMES = LEFT_HAND_NAMES0[HAND_X_IDXS]\nHAND_Y_NAMES = LEFT_HAND_NAMES0[HAND_Y_IDXS]\n\nprint(f'HAND_X_NAMES: {HAND_X_NAMES}')","metadata":{"papermill":{"duration":0.045648,"end_time":"2023-07-02T11:33:49.488024","exception":false,"start_time":"2023-07-02T11:33:49.442376","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:13.890132Z","iopub.execute_input":"2023-09-24T16:36:13.890722Z","iopub.status.idle":"2023-09-24T16:36:13.901396Z","shell.execute_reply.started":"2023-09-24T16:36:13.890688Z","shell.execute_reply":"2023-09-24T16:36:13.900450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mean/STD Loading\n\n\n","metadata":{"papermill":{"duration":0.037446,"end_time":"2023-07-02T11:33:49.563172","exception":false,"start_time":"2023-07-02T11:33:49.525726","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Mean/Standard Deviations of data used for normalizing\nMEANS = np.load('/kaggle/input/aslfr-eda-preprocessing-dataset-for-beginners/MEANS.npy').reshape(-1)\nSTDS = np.load('/kaggle/input/aslfr-eda-preprocessing-dataset-for-beginners/STDS.npy').reshape(-1)\n\nprint(f'First 5 values of MEANS: {MEANS[:5]}')\nprint(f'Shape of MEANS: {MEANS.shape}') # 164 values, 1 for every column","metadata":{"papermill":{"duration":0.051188,"end_time":"2023-07-02T11:33:49.648816","exception":false,"start_time":"2023-07-02T11:33:49.597628","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:13.903005Z","iopub.execute_input":"2023-09-24T16:36:13.903461Z","iopub.status.idle":"2023-09-24T16:36:13.924072Z","shell.execute_reply.started":"2023-09-24T16:36:13.903430Z","shell.execute_reply":"2023-09-24T16:36:13.922666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's add the preprocessing layer again here, as we will use it in the `TFLite` model.","metadata":{}},{"cell_type":"code","source":"class PreprocessLayer(tf.keras.layers.Layer):\n    def __init__(self):\n        super(PreprocessLayer, self).__init__()\n        self.normalisation_correction = tf.constant(\n                    # Add 0.50 to x coordinates of left hand (original right hand) and substract 0.50 of right hand (original left hand)\n                     [0.50 if 'x' in name else 0.00 for name in LEFT_HAND_NAMES0],\n                dtype=tf.float32,\n            )\n    \n    @tf.function(\n        input_signature=(tf.TensorSpec(shape=[None,N_COLS0], dtype=tf.float32),),\n    )\n    def call(self, data0, resize=True):\n        # Fill NaN Values With 0\n        data = tf.where(tf.math.is_nan(data0), 0.0, data0)\n        \n        # Hacky\n        data = data[None]\n        \n        # Empty Hand Frame Filtering\n        hands = tf.slice(data, [0,0,0], [-1, -1, 84])\n        hands = tf.abs(hands)\n        mask = tf.reduce_sum(hands, axis=2)\n        mask = tf.not_equal(mask, 0)\n        data = data[mask][None]\n        \n        # Pad Zeros\n        N_FRAMES = len(data[0])\n        if N_FRAMES < N_TARGET_FRAMES:\n            data = tf.concat((\n                data,\n                tf.zeros([1,N_TARGET_FRAMES-N_FRAMES,N_COLS], dtype=tf.float32)\n            ), axis=1)\n        # Downsample\n        data = tf.image.resize(\n            data,\n            [1, N_TARGET_FRAMES],\n            method=tf.image.ResizeMethod.BILINEAR,\n        )\n        \n        # Squeeze Batch Dimension\n        data = tf.squeeze(data, axis=[0])\n        \n        return data\n    \npreprocess_layer = PreprocessLayer()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:36:13.926092Z","iopub.execute_input":"2023-09-24T16:36:13.926673Z","iopub.status.idle":"2023-09-24T16:36:16.415953Z","shell.execute_reply.started":"2023-09-24T16:36:13.926640Z","shell.execute_reply":"2023-09-24T16:36:16.414891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Dataset\n\n1. `get_train_dataset` is a Python function that creates a generator for the training dataset.\n\n2. `sample_idxs` is an array of indices ranging from 0 to the length of the training dataset `X`. These indices represent the samples in the dataset.\n\n3. The function enters an infinite loop using `while True`, allowing it to continually provide batches of training samples.\n\n4. In each iteration of the loop:\n   - `random_sample_idxs` is generated by randomly selecting `batch_size` indices from `sample_idxs`. This step simulates random sampling of training samples with replacement.\n   - Two dictionaries, `inputs` and `outputs`, are created. `inputs` contains two key-value pairs:\n     - `'frames'`: This key corresponds to the input data, represented by `X` (frames or sequences of numerical data).\n     - `'phrase'`: This key corresponds to the output data (labels), represented by `y` (phrases or sequences of integer indices).\n   - `outputs` is a batch of output data represented by `y` based on the random indices selected.\n\n5. The `yield` statement is used to return the `inputs` and `outputs` dictionaries as a pair of values in each iteration of the generator. This allows the generator to yield a batch of training samples in each iteration.\n\n6. The generator, when called, creates an iterator that can be used in a `for` loop to iterate over training batches. Each batch is represented by `inputs` and `outputs`.\n\n7. After defining the generator, it is used to create the `train_dataset`. This dataset will be used during model training.\n\n8. Finally, the variable `TRAIN_STEPS_PER_EPOCH` is calculated. It represents the number of training steps required per epoch based on the batch size and the total number of training samples.\n\nThe generator and dataset iterator provide an efficient way to iterate over the training dataset in batches, which is necessary for training machine learning models on large datasets. In each training step, the model can process a batch of samples, updating its weights and gradually improving its performance.","metadata":{"papermill":{"duration":0.023965,"end_time":"2023-07-02T11:33:52.934821","exception":false,"start_time":"2023-07-02T11:33:52.910856","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Train Dataset Iterator\ndef get_train_dataset(X, y, batch_size=BATCH_SIZE):\n    sample_idxs = np.arange(len(X))\n    while True:\n        # Get random indices\n        random_sample_idxs = np.random.choice(sample_idxs, batch_size)\n        \n        inputs = {\n            'frames': X[random_sample_idxs],\n            'phrase': y[random_sample_idxs],\n        }\n        outputs = y[random_sample_idxs]\n        \n        yield inputs, outputs\n        \n\n# Train Dataset\ntrain_dataset = get_train_dataset(X_train, y_train)\n\n# Training Steps Per Epoch\nTRAIN_STEPS_PER_EPOCH = math.ceil(N_TRAIN_SAMPLES / BATCH_SIZE)\nprint(f'TRAIN_STEPS_PER_EPOCH: {TRAIN_STEPS_PER_EPOCH}')","metadata":{"papermill":{"duration":0.033186,"end_time":"2023-07-02T11:33:52.992213","exception":false,"start_time":"2023-07-02T11:33:52.959027","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:16.418381Z","iopub.execute_input":"2023-09-24T16:36:16.418977Z","iopub.status.idle":"2023-09-24T16:36:16.427044Z","shell.execute_reply.started":"2023-09-24T16:36:16.418941Z","shell.execute_reply":"2023-09-24T16:36:16.426035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation Dataset\n\n1. `get_val_dataset` is a Python function that creates a generator for the validation dataset. This generator will be used to provide batches of validation samples for evaluating the model during training.\n\n2. `offsets` is an array of offsets representing the starting indices of each batch within the validation dataset. These offsets are generated using `np.arange` and ensure that the entire validation dataset is covered.\n\n3. The function enters an infinite loop using `while True`, allowing it to continually provide batches of validation samples.\n\n4. In each iteration of the outer loop, the generator iterates over the entire validation set by iterating through the `offsets` array. This inner loop covers all batches of the validation dataset.\n\n5. For each batch, two dictionaries, `inputs` and `outputs`, are created. `inputs` contains two key-value pairs:\n   - `'frames'`: This key corresponds to the input data, represented by `X` (frames or sequences of numerical data).\n   - `'phrase'`: This key corresponds to the output data (labels), represented by `y` (phrases or sequences of integer indices).\n\n6. `outputs` is a batch of output data represented by `y` for the current batch.\n\n7. The `yield` statement is used to return the `inputs` and `outputs` dictionaries as a pair of values in each iteration of the generator. This allows the generator to yield a batch of validation samples in each iteration.\n\n8. After defining the generator, it can be used to create the `val_dataset`, which will be used for validation during model training.\n\n9. If the variable `USE_VAL` is `True`, indicating that a validation dataset should be used, the `val_dataset` is created.\n\n10. Finally, if a validation dataset is used (`USE_VAL` is `True`), the variable `N_VAL_STEPS_PER_EPOCH` is calculated. It represents the number of validation steps required per epoch based on the batch size and the total number of validation samples.\n\nThe generator and validation dataset iterator provide an efficient way to iterate over the validation dataset in batches, which is necessary for evaluating the model's performance during training. In each validation step, the model can process a batch of validation samples and compute metrics such as loss and accuracy to monitor its performance.","metadata":{"papermill":{"duration":0.02478,"end_time":"2023-07-02T11:33:53.161179","exception":false,"start_time":"2023-07-02T11:33:53.136399","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Validation Set\ndef get_val_dataset(X, y, batch_size=BATCH_SIZE):\n    offsets = np.arange(0, len(X), batch_size)\n    while True:\n        # Iterate over whole validation set\n        for offset in offsets:\n            inputs = {\n                'frames': X[offset:offset+batch_size],\n                'phrase': y[offset:offset+batch_size],\n            }\n            outputs = y[offset:offset+batch_size]\n\n            yield inputs, outputs\n            \n\n# Validation Dataset\nif USE_VAL:\n    val_dataset = get_val_dataset(X_val, y_val)\n    \n\nif USE_VAL:\n    N_VAL_STEPS_PER_EPOCH = math.ceil(N_VAL_SAMPLES / BATCH_SIZE)\n    print(f'N_VAL_STEPS_PER_EPOCH: {N_VAL_STEPS_PER_EPOCH}')","metadata":{"papermill":{"duration":0.0344,"end_time":"2023-07-02T11:33:53.220231","exception":false,"start_time":"2023-07-02T11:33:53.185831","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:16.428656Z","iopub.execute_input":"2023-09-24T16:36:16.429053Z","iopub.status.idle":"2023-09-24T16:36:16.443580Z","shell.execute_reply.started":"2023-09-24T16:36:16.429021Z","shell.execute_reply":"2023-09-24T16:36:16.442585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Config\n\nHere we defines several important variables related to the configuration and architecture of a Transformer model:\n\n1. `LAYER_NORM_EPS`: This variable represents the epsilon value used in the layer normalization of the Transformer model. Layer normalization is a technique to normalize the activations of a layer, and `LAYER_NORM_EPS` is the small constant added to the denominator to avoid division by zero.\n\n2. `UNITS_ENCODER` and `UNITS_DECODER`: These variables indicate the size (dimensionality) of the final output and the embeddings of the encoder and decoder, respectively. The encoder and decoder layers will have output and embedding dimensions defined by these variables.\n\n3. `NUM_BLOCKS_ENCODER` and `NUM_BLOCKS_DECODER`: These variables represent the number of blocks (layers) in the encoder and decoder of the Transformer model. Each block consists of multiple sub-layers, including multi-head attention and feed-forward layers.\n\n4. `NUM_HEADS`: Indicates the number of attention heads in the multi-head attention mechanism of the Transformer. Multi-head attention allows the model to attend to different parts of the input simultaneously, improving its ability to capture complex patterns.\n\n5. `MLP_RATIO`: This variable is the multiplication factor used to calculate the size of the feed-forward layer in the Transformer block. The feed-forward layer is a part of the self-attention mechanism and its size is determined by multiplying the `UNITS_ENCODER` or `UNITS_DECODER` by `MLP_RATIO`.\n\n6. `EMBEDDING_DROPOUT`, `MLP_DROPOUT_RATIO`, `MHA_DROPOUT_RATIO`, and `CLASSIFIER_DROPOUT_RATIO`: These variables define the dropout rates used in different parts of the model. Dropout is a regularization technique that helps prevent overfitting by randomly setting a fraction of input units to zero during training. These variables control the dropout rates for the embedding layer, the multi-layer perceptron (MLP) layer, the multi-head attention (MHA) mechanism, and the classifier layer, respectively.\n\n7. `INIT_HE_UNIFORM`, `INIT_GLOROT_UNIFORM`, and `INIT_ZEROS`: These variables specify the initializers used to initialize the weights of the model's layers. Initialization is crucial for the training process, and different initializers may affect how well the model converges during training. `INIT_HE_UNIFORM` uses He Uniform initialization, `INIT_GLOROT_UNIFORM` uses Glorot Uniform initialization, and `INIT_ZEROS` initializes the weights with zeros.\n\n8. `GELU`: `GELU` is an activation function called Gaussian Error Linear Unit. It is used as the activation function in the model. GELU is known to perform well in deep neural networks and is often used in Transformer models.\n\nThese variables collectively define the hyperparameters, architecture, and regularization techniques used in the Transformer model. They play a crucial role in shaping the model's behavior, capacity, and training dynamics. Properly tuning these hyperparameters is essential for achieving good model performance on the target task.","metadata":{"papermill":{"duration":0.0243,"end_time":"2023-07-02T11:33:53.381866","exception":false,"start_time":"2023-07-02T11:33:53.357566","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Epsilon value for layer normalisation\nLAYER_NORM_EPS = 1e-6\n\n# final embedding and transformer embedding size\nUNITS_ENCODER = 384\nUNITS_DECODER = 256\n\n# Transformer\nNUM_BLOCKS_ENCODER = 4\nNUM_BLOCKS_DECODER = 2\nNUM_HEADS = 4\nMLP_RATIO = 2\n\n# Dropout\nEMBEDDING_DROPOUT = 0.05\nMLP_DROPOUT_RATIO = 0.30\nMHA_DROPOUT_RATIO = 0.15\nCLASSIFIER_DROPOUT_RATIO = 0.10\n\n# Initiailizers\nINIT_HE_UNIFORM = tf.keras.initializers.he_uniform\nINIT_GLOROT_UNIFORM = tf.keras.initializers.glorot_uniform\nINIT_ZEROS = tf.keras.initializers.constant(0.0)\n# Activations\nGELU = tf.keras.activations.gelu","metadata":{"papermill":{"duration":0.036845,"end_time":"2023-07-02T11:33:53.443416","exception":false,"start_time":"2023-07-02T11:33:53.406571","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:16.445178Z","iopub.execute_input":"2023-09-24T16:36:16.445591Z","iopub.status.idle":"2023-09-24T16:36:16.454218Z","shell.execute_reply.started":"2023-09-24T16:36:16.445560Z","shell.execute_reply":"2023-09-24T16:36:16.453323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Landmark Embedding\n\nThe `LandmarkEmbedding` class is a custom layer in TensorFlow/Keras that is designed to embed landmarks using fully connected layers. This custom layer takes a sequence of landmark data as input and returns embedded representations for each landmark.\n\n1. **Initialization**: The `__init__` method initializes the `LandmarkEmbedding` layer. It takes two arguments, `units` and `name`. `units` specifies the dimensionality of the embedded representation, and `name` sets the name of the layer. It also sets `supports_masking` to `True`, indicating that the layer supports masking.\n\n2. **Build Method**: The `build` method is called to create the layer's weights. In this method, two important components are defined:\n   - `empty_embedding`: This component is a trainable weight representing the embedding for missing landmarks in a frame. It's initialized with zeros using the `INIT_ZEROS` initializer.\n   - `dense`: This component is a sequential model consisting of two dense (fully connected) layers. The first dense layer uses the GELU activation function and Glorot Uniform initialization (`INIT_GLOROT_UNIFORM`). The second dense layer uses He Uniform initialization (`INIT_HE_UNIFORM`). These layers are used to embed the landmark data.\n\n3. **Call Method**: The `call` method is where the actual embedding process takes place. It accepts the input `x`, which is expected to be a tensor representing the landmark data. The following steps are performed in the `call` method:\n   - It checks whether each landmark is missing in the frame by summing along the third axis (axis=2) and creating a binary mask (0 if missing, 1 if present).\n   - If a landmark is missing (indicated by a sum of 0 in the mask), it uses the `empty_embedding` for that landmark, effectively returning zeros.\n   - If a landmark is present, it passes the landmark data through the `dense` layers to obtain the embedded representation.\n\nThis custom layer allows you to handle missing landmarks by providing an empty embedding for them and embedding the available landmarks using a neural network. It's a useful component in a model that processes sequences of landmarks, such as the one you're building.","metadata":{"papermill":{"duration":0.024508,"end_time":"2023-07-02T11:33:53.492645","exception":false,"start_time":"2023-07-02T11:33:53.468137","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Embeds a landmark using fully connected layers\nclass LandmarkEmbedding(tf.keras.Model):\n    def __init__(self, units, name):\n        super(LandmarkEmbedding, self).__init__(name=f'{name}_embedding')\n        self.units = units\n        self.supports_masking = True\n        \n    def build(self, input_shape):\n        # Embedding for missing landmark in frame, initialized with zeros\n        self.empty_embedding = self.add_weight(\n            name=f'{self.name}_empty_embedding',\n            shape=[self.units],\n            initializer=INIT_ZEROS,)\n        # Embedding: 2 dense layers\n        self.dense = tf.keras.Sequential([\n            tf.keras.layers.Dense(self.units, name=f'{self.name}_dense_1', use_bias=False, kernel_initializer=INIT_GLOROT_UNIFORM, activation=GELU),\n            tf.keras.layers.Dense(self.units, name=f'{self.name}_dense_2', use_bias=False, kernel_initializer=INIT_HE_UNIFORM),\n        ], name=f'{self.name}_dense')\n\n    def call(self, x):\n        # if the landmark = 0 -> use empty embedding (return 0s), else use dense embedding\n        return tf.where(\n                # Checks whether landmark is missing in frame\n                tf.reduce_sum(x, axis=2, keepdims=True) == 0,\n                # If so, the empty embedding is used\n                self.empty_embedding,\n                # Otherwise the landmark data is embedded\n                self.dense(x),\n            )","metadata":{"papermill":{"duration":0.03711,"end_time":"2023-07-02T11:33:53.554368","exception":false,"start_time":"2023-07-02T11:33:53.517258","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:16.455965Z","iopub.execute_input":"2023-09-24T16:36:16.456304Z","iopub.status.idle":"2023-09-24T16:36:16.466638Z","shell.execute_reply.started":"2023-09-24T16:36:16.456256Z","shell.execute_reply":"2023-09-24T16:36:16.465676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Embedding\n\nThe `Embedding` class is another custom layer in TensorFlow/Keras that is used to create embeddings for each frame in a sequence. It incorporates both positional embeddings and landmark embeddings.\n\n1. **Initialization**: The `__init__` method initializes the `Embedding` layer. It sets `supports_masking` to `True`, indicating that the layer supports masking. The masking feature can be useful when dealing with sequences of varying lengths.\n\n2. **Build Method**: The `build` method is responsible for creating the layer's weights. In this method, two important components are defined:\n\n   - `positional_embedding`: This component represents the positional embeddings for each frame index. It is a trainable variable initialized with zeros but is trainable. The shape of this variable is `[N_TARGET_FRAMES, UNITS_ENCODER]`, where `N_TARGET_FRAMES` is the number of target frames (presumably the number of frames in the output sequence), and `UNITS_ENCODER` is the dimensionality of the embedding.\n\n   - `dominant_hand_embedding`: This component is an instance of the `LandmarkEmbedding` layer that you defined earlier. It is used to embed the landmark data for the dominant hand. The embedding dimension is set to `UNITS_ENCODER`.\n\n3. **Call Method**: The `call` method is where the embedding process occurs. It accepts the input `x`, which is expected to be a tensor representing the frames of data. The following steps are performed in the `call` method:\n\n   - Data Normalization: The input data `x` is normalized. Specifically, it checks whether each element is equal to 0.0 (indicating missing data) and replaces those elements with 0.0. For non-missing data, it performs a standardization operation using the `MEANS` and `STDS` constants. This normalization step scales the data to have zero mean and unit variance.\n\n   - Dominant Hand Embedding: The normalized data is passed through the `dominant_hand_embedding` layer, which applies landmark embeddings. This step extracts information from the landmarks.\n\n   - Positional Encoding: The positional embeddings are added to the result of the dominant hand embedding. This adds positional information to each frame, which can be crucial for capturing temporal dependencies in the sequence.\n\n   - The final embedded representation is returned.\n\nThis custom layer combines both landmark and positional embeddings to create meaningful representations for each frame in the input sequence. These embeddings can then be used as input to subsequent layers in your model.","metadata":{"papermill":{"duration":0.023829,"end_time":"2023-07-02T11:33:53.60253","exception":false,"start_time":"2023-07-02T11:33:53.578701","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Creates embedding for each frame\nclass Embedding(tf.keras.Model):\n    def __init__(self):\n        super(Embedding, self).__init__()\n        self.supports_masking = True\n    \n    def build(self, input_shape):\n        # Positional embedding for each frame index\n        self.positional_embedding = tf.Variable(\n            initial_value=tf.zeros([N_TARGET_FRAMES, UNITS_ENCODER], dtype=tf.float32),\n            trainable=True,\n            name='embedding_positional_encoder',)\n        # Embedding layer for Landmarks\n        self.dominant_hand_embedding = LandmarkEmbedding(UNITS_ENCODER, 'dominant_hand')\n\n    def call(self, x, training=False):\n        # Normalize data before aplying embedding\n        x = tf.where(\n                tf.math.equal(x, 0.0),\n                0.0,\n                (x - MEANS) / STDS,)\n        # Dominant Hand: apply landmark embedding to extract information\n        x = self.dominant_hand_embedding(x)\n        # Add Positional Encoding\n        x = x + self.positional_embedding\n        \n        return x","metadata":{"papermill":{"duration":0.036321,"end_time":"2023-07-02T11:33:53.663162","exception":false,"start_time":"2023-07-02T11:33:53.626841","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:16.475556Z","iopub.execute_input":"2023-09-24T16:36:16.475815Z","iopub.status.idle":"2023-09-24T16:36:16.482932Z","shell.execute_reply.started":"2023-09-24T16:36:16.475791Z","shell.execute_reply":"2023-09-24T16:36:16.481868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `PositionalEmbedding` class is designed to perform positional embedding, which adds positional information to the input data. This positional information can be critical for capturing the sequential order of elements in a sequence. Let's break down this class:\n\n1. **Initialization**: The `__init__` method initializes the `PositionalEmbedding` layer. It doesn't have any specific configuration parameters in this case.\n\n2. **Build Method**: The `build` method is responsible for creating the layer's weights. In this case, it defines a `positional_embedding` variable. This variable represents the positional embeddings and is trainable. The shape of this variable is `[1, 164]`, which indicates that it contains positional embeddings for 164 positions. The choice of 164 positions is specific to your use case and the length of sequences you are dealing with.\n\n3. **Call Method**: The `call` method is where the positional embedding process occurs. It accepts an input tensor `x`, which typically represents the data to which positional embeddings need to be added. The following steps are performed in the `call` method:\n\n   - The input tensor `x` is added to the `positional_embedding` variable. This operation effectively adds the positional information to each element of the input tensor.\n\n   - The resulting tensor, which now includes positional embeddings, is returned.\n\nPositional embeddings are a key component in models like Transformers because they provide information about the order of elements in a sequence. These embeddings are often added to the input data at the beginning of the model to ensure that the model can capture the sequential dependencies effectively.\n\nTo use this `PositionalEmbedding` layer in your model, you would typically include it as one of the initial layers in your model architecture to embed the input sequence with positional information before further processing.","metadata":{}},{"cell_type":"code","source":"# positional Embedding class\nclass PositionalEmbedding(tf.keras.Model):\n    def __init__(self):\n        super(PositionalEmbedding, self).__init__()\n    \n    def build(self, input_shape):\n        self.positional_embedding = tf.Variable(\n            initial_value=tf.zeros([1, 164], dtype=tf.float32),\n            trainable=True,\n            name='positional_embedding')\n    \n    def call(self, x):\n        return x + self.positional_embedding","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:36:16.484617Z","iopub.execute_input":"2023-09-24T16:36:16.484952Z","iopub.status.idle":"2023-09-24T16:36:16.495597Z","shell.execute_reply.started":"2023-09-24T16:36:16.484921Z","shell.execute_reply":"2023-09-24T16:36:16.494587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tranformer\n\nThe `MultiHeadAttention` layer is an implementation of the multi-head self-attention mechanism used in Transformer models. This mechanism is crucial for capturing relationships between different elements in a sequence. Let's break down the key components of this layer:\n\n1. **Initialization**: In the `__init__` method, you specify the parameters for the multi-head attention layer:\n   - `d_model`: The total dimensionality of the model or the input data.\n   - `n_heads`: The number of attention heads.\n   - `dropout`: The dropout rate applied to the output of the attention layer.\n   - `d_out`: The dimensionality of the output after the attention layer (if different from `d_model`).\n\n2. **Learnable Projections**: Three learnable projections are defined for queries (`wq`), keys (`wk`), and values (`wv`). Each of these projections uses a fused multi-head approach, meaning that a single dense layer is applied to all attention heads, and then the results are reshaped and permuted to match the multi-head attention format.\n\n3. **Output Projection**: The output of the multi-head attention mechanism is projected to the specified output dimensionality (`d_model` or `d_out`) using a dense layer (`wo`). This projection allows the model to adapt the output of the attention mechanism to the desired dimensionality.\n\n4. **Softmax Activation with Masking**: The attention scores are computed using a dot product between queries and keys (`tf.matmul(Q, K, transpose_b=True) * self.scale`) and then passed through a softmax activation (`self.softmax`) with masking support. This masking is essential for models that work with sequences of varying lengths, as it ensures that padded elements do not contribute to the attention scores.\n\n5. **Reshaping**: The attention scores are reshaped to flatten the attention heads before applying the output projection.\n\n6. **Dropout**: A dropout layer (`self.do`) is applied to the output of the attention mechanism for regularization.\n\n7. **Support for Masking**: The layer supports masking through the `mask` parameter when applying the softmax activation.\n\n8. **`call` Method**: The `call` method defines the forward pass of the layer. It takes queries (`q`), keys (`k`), and values (`v`) as inputs, along with an optional `attention_mask`. The method computes the attention scores, applies the softmax activation with masking, and produces the output after projection and dropout.\n\nThis implementation follows the typical structure of multi-head self-attention layers found in Transformer models. It's important for capturing dependencies between different parts of the input sequence. This layer can be used as one of the building blocks when constructing a Transformer-based model for various natural language processing tasks, sequence-to-sequence tasks, and more.","metadata":{}},{"cell_type":"code","source":"# based on: https://stackoverflow.com/questions/67342988/verifying-the-implementation-of-multihead-attention-in-transformer\n# replaced softmax with softmax layer to support masked softmax\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, n_heads, dropout, d_out=None):\n        super(MultiHeadAttention,self).__init__()\n        # Number of Units in Model\n        self.d_model = d_model\n        # Number of Attention Heads\n        self.n_heads = n_heads\n        # Number of Units in Intermediate Layers\n        self.depth = d_model // 2\n        # Scaling Factor Of Values\n        self.scale = 1.0 / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n        # Learnable Projections to Depth\n        self.wq = self.fused_mha(self.depth)\n        self.wk = self.fused_mha(self.depth)\n        self.wv = self.fused_mha(self.depth)\n        # Output Projection\n        self.wo = tf.keras.layers.Dense(d_model if d_out is None else d_out, use_bias=False)\n        # Softmax Activation Which Supports Masking\n        self.softmax = tf.keras.layers.Softmax()\n        # Reshaping Of Multiple Attention heads to Single Value\n        self.reshape = tf.keras.Sequential([\n            # [attention heads, number of frames, d_model] → [number of frames, n_heads, d_model // n_heads]\n            tf.keras.layers.Permute([2, 1, 3]),\n            # [number of frames, attention heads, d_model] → [number of frames, d_model]\n            tf.keras.layers.Reshape([N_TARGET_FRAMES, self.depth]),\n        ])\n        # Output Dropout\n        self.do = tf.keras.layers.Dropout(dropout)\n        self.supports_masking = True\n        \n    # Single dense layer for all attention heads\n    def fused_mha(self, dim):\n        return tf.keras.Sequential([\n            # Single dense layer\n            tf.keras.layers.Dense(dim, use_bias=False),\n            # Reshape to [number of frames, number of attention head, depth]\n            tf.keras.layers.Reshape([N_TARGET_FRAMES, self.n_heads, dim // self.n_heads]),\n            # Permutate to [number of attention heads, number of frames, depth]\n            tf.keras.layers.Permute([2, 1, 3]),\n        ])\n        \n    def call(self, q, k, v, attention_mask=None, training=False):\n        # Projections to attention heads\n        Q = self.wq(q)\n        K = self.wk(k)\n        V = self.wv(v)\n        # Matrix multiply QxK to acquire attention scores\n        x = tf.matmul(Q, K, transpose_b=True) * self.scale\n        # Softmax attention scores and Multiply with Values\n        x = self.softmax(x, mask=attention_mask) @ V\n        # Reshape to flatten attention heads\n        x = self.reshape(x)\n        # Output projection\n        x = self.wo(x)\n        # Dropout\n        x = self.do(x, training=training)\n        return x","metadata":{"papermill":{"duration":0.04073,"end_time":"2023-07-02T11:33:53.779389","exception":false,"start_time":"2023-07-02T11:33:53.738659","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:16.498873Z","iopub.execute_input":"2023-09-24T16:36:16.499137Z","iopub.status.idle":"2023-09-24T16:36:16.514814Z","shell.execute_reply.started":"2023-09-24T16:36:16.499113Z","shell.execute_reply":"2023-09-24T16:36:16.513842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoder\n\nThe `Encoder` class represents the encoder part of a Transformer-based model. The encoder processes the input data, captures relevant information, and prepares it for further processing by the decoder. Here's an explanation of the key components of this `Encoder` class:\n\n1. **Initialization**: In the `__init__` method, you specify the number of transformer blocks (`num_blocks`) that make up the encoder. Each block consists of multi-head self-attention and feed-forward layers. You also set `supports_masking` to `True` to indicate that the encoder supports masking.\n\n2. **Building Blocks**: In the `build` method, you create the layers that make up each transformer block (`num_blocks`). For each block, you define the following components:\n   - Layer Normalization (`ln_1` and `ln_2`): Layer normalization is applied before and after the multi-head self-attention layer and the feed-forward layer in each block. Layer normalization helps stabilize the training process.\n   - Multi-Head Self-Attention (`mha`): The multi-head self-attention layer processes the input data, capturing dependencies between different elements in the sequence.\n   - Multi-Layer Perception (`mlp`): The feed-forward layer applies a series of dense layers with activation functions (GELU) and dropout to transform the data.\n\n3. **Optional Dimensionality Projection**: You provide an option to project the output of the encoder to a different dimensionality (`UNITS_DECODER`) if it's different from the encoder's dimensionality (`UNITS_ENCODER`). This projection can be useful when the encoder and decoder have different dimensionalities.\n\n4. **Attention Mask**: You define the `get_attention_mask` method to create an attention mask that helps the model ignore missing frames in the input data. This mask is applied to the multi-head self-attention layer.\n\n5. **Forward Pass**: In the `call` method, you perform the forward pass through the encoder. Here are the steps:\n   - Calculate the attention mask using the `get_attention_mask` method.\n   - Iterate through the transformer blocks, applying layer normalization, multi-head self-attention, and feed-forward layers in sequence.\n   - Optionally project the output to the decoder's dimensionality (`UNITS_DECODER`) if specified.\n\nThe `Encoder` class can be used as part of a larger Transformer-based model. It takes input data (`x`) and information about the input frames (`x_inp`) to process and capture relationships within the sequence. The encoder's output is typically passed to the decoder for further processing in tasks like sequence-to-sequence modeling or language translation.\n\nThis implementation follows the standard structure of the encoder in a Transformer model and can be customized as needed for specific applications.","metadata":{"papermill":{"duration":0.024245,"end_time":"2023-07-02T11:33:53.828275","exception":false,"start_time":"2023-07-02T11:33:53.80403","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Encoder based on multiple transformer blocks\nclass Encoder(tf.keras.Model):\n    def __init__(self, num_blocks):\n        super(Encoder, self).__init__(name='encoder')\n        self.num_blocks = num_blocks\n        self.supports_masking = True\n    \n    def build(self, input_shape):\n        self.ln_1s = []\n        self.mhas = []\n        self.ln_2s = []\n        self.mlps = []\n        # Make Transformer Blocks\n        for i in range(self.num_blocks):\n            # First Layer Normalisation\n            self.ln_1s.append(tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS))\n            # Multi Head Attention\n            self.mhas.append(MultiHeadAttention(UNITS_ENCODER, NUM_HEADS, MHA_DROPOUT_RATIO))\n            # Second Layer Normalisation\n            self.ln_2s.append(tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS))\n            # Multi Layer Perception\n            self.mlps.append(tf.keras.Sequential([\n                tf.keras.layers.Dense(UNITS_ENCODER * MLP_RATIO, activation=GELU, kernel_initializer=INIT_GLOROT_UNIFORM, use_bias=False),\n                tf.keras.layers.Dropout(MLP_DROPOUT_RATIO),\n                tf.keras.layers.Dense(UNITS_ENCODER, kernel_initializer=INIT_HE_UNIFORM, use_bias=False),\n            ]))\n            # Optional Projection to Decoder Dimension\n            if UNITS_ENCODER != UNITS_DECODER:\n                self.dense_out = tf.keras.layers.Dense(UNITS_DECODER, kernel_initializer=INIT_GLOROT_UNIFORM, use_bias=False)\n                self.apply_dense_out = True\n            else:\n                self.apply_dense_out = False\n                \n    def get_attention_mask(self, x_inp):\n        # Attention Mask\n        attention_mask = tf.math.count_nonzero(x_inp, axis=[2], keepdims=True, dtype=tf.int32)\n        attention_mask = tf.math.count_nonzero(attention_mask, axis=[2], keepdims=False)\n        attention_mask = tf.expand_dims(attention_mask, axis=1)\n        attention_mask = tf.expand_dims(attention_mask, axis=1)\n        return attention_mask\n        \n    def call(self, x, x_inp, training=False):\n        # Attention mask to ignore missing frames\n        attention_mask = self.get_attention_mask(x_inp)\n        # Iterate input over transformer blocks\n        for ln_1, mha, ln_2, mlp in zip(self.ln_1s, self.mhas, self.ln_2s, self.mlps):\n            x = ln_1(x + mha(x, x, x, attention_mask=attention_mask))\n            x = ln_2(x + mlp(x))\n            \n        # Optional Projection to Decoder Dimension\n        if self.apply_dense_out:\n            x = self.dense_out(x)\n    \n        return x","metadata":{"papermill":{"duration":0.041438,"end_time":"2023-07-02T11:33:53.894476","exception":false,"start_time":"2023-07-02T11:33:53.853038","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:16.517239Z","iopub.execute_input":"2023-09-24T16:36:16.518062Z","iopub.status.idle":"2023-09-24T16:36:16.532398Z","shell.execute_reply.started":"2023-09-24T16:36:16.518028Z","shell.execute_reply":"2023-09-24T16:36:16.531761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decoder\n\nThe `Decoder` class represents the decoder part of a Transformer-based model. The decoder takes the encoder's output, previously generated tokens, and positional information to generate the next token in a sequence. Here's an explanation of the key components of this `Decoder` class:\n\n1. **Initialization**: In the `__init__` method, you specify the number of transformer blocks (`num_blocks`) that make up the decoder. Each block consists of multi-head self-attention and feed-forward layers. You also set `supports_masking` to `True` to indicate that the decoder supports masking.\n\n2. **Causal Attention Mask**: You create a causal attention mask using the `get_causal_attention_mask` method. This mask ensures that each position in the sequence can only attend to previous positions, preventing information flow from future tokens.\n\n3. **Positional Embedding**: You initialize a positional embedding variable, which represents the positional information of tokens in the decoder. This embedding is added to the character embeddings.\n\n4. **Character Embedding**: You apply an embedding layer to the input character sequence (`phrase`). This layer converts character indices into dense vectors.\n\n5. **Positional Encoder MHA**: You use a multi-head attention mechanism (`pos_emb_mha`) to capture relationships between tokens in the decoder, considering both the character embeddings and positional embeddings. Layer normalization is applied after the multi-head attention.\n\n6. **Building Blocks**: Similar to the encoder, you create the layers that make up each transformer block (`num_blocks`). For each block, you define the following components:\n   - Layer Normalization (`ln_1` and `ln_2`): Layer normalization is applied before and after the multi-head self-attention layer and the feed-forward layer in each block.\n   - Multi-Head Self-Attention (`mha`): The multi-head self-attention layer processes the decoder input, capturing dependencies between different elements in the sequence.\n   - Multi-Layer Perception (`mlp`): The feed-forward layer applies a series of dense layers with activation functions (GELU) and dropout to transform the data.\n\n7. **Input Preparation**: You prepare the input data for the decoder, including adding a start-of-sequence (SOS) token at the beginning and padding with pad tokens. This is done to align the input with the expected output sequence.\n\n8. **Attention Mask**: You define the `get_attention_mask` method to create an attention mask that helps the model ignore missing frames in the input data. This mask is applied to the multi-head self-attention layer.\n\n9. **Forward Pass**: In the `call` method, you perform the forward pass through the decoder. Here are the steps:\n   - Prepend an SOS token to the input character sequence.\n   - Pad the sequence with pad tokens to ensure a fixed length.\n   - Add positional embeddings to the character embeddings.\n   - Apply causal attention using the causal attention mask.\n   - Calculate the attention mask for the input frames using the `get_attention_mask` method.\n   - Iterate through the transformer blocks, applying layer normalization, multi-head self-attention, and feed-forward layers in sequence.\n   - Slice the output to retain only the first 31 characters, corresponding to the output phrase.\n\nThe `Decoder` class can be used as part of a larger Transformer-based model for sequence-to-sequence tasks where the goal is to generate an output sequence (e.g., translation or text generation) based on an input sequence. It integrates both character and positional information and generates output tokens one step at a time while considering the causal dependencies between tokens.","metadata":{"papermill":{"duration":0.024624,"end_time":"2023-07-02T11:33:53.943518","exception":false,"start_time":"2023-07-02T11:33:53.918894","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Decoder based on multiple transformer blocks\nclass Decoder(tf.keras.Model):\n    def __init__(self, num_blocks):\n        super(Decoder, self).__init__(name='decoder')\n        self.num_blocks = num_blocks\n        self.supports_masking = True\n    \n    def build(self, input_shape):\n        # Causal Mask Batch Size 1\n        self.causal_mask = self.get_causal_attention_mask()\n        # Positional Embedding, initialized with zeros\n        self.positional_embedding = tf.Variable(\n            initial_value=tf.zeros([N_TARGET_FRAMES, UNITS_DECODER], dtype=tf.float32),\n            trainable=True,\n            name='embedding_positional_encoder',\n        )\n        # Character Embedding\n        self.char_emb = tf.keras.layers.Embedding(N_UNIQUE_CHARACTERS, UNITS_DECODER, embeddings_initializer=INIT_ZEROS)\n        # Positional Encoder MHA\n        self.pos_emb_mha = MultiHeadAttention(UNITS_DECODER, NUM_HEADS, MHA_DROPOUT_RATIO)\n        self.pos_emb_ln = tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n        # First Layer Normalisation\n        self.ln_1s = []\n        self.mhas = []\n        self.ln_2s = []\n        self.mlps = []\n        # Make Transformer Blocks\n        for i in range(self.num_blocks):\n            # First Layer Normalisation\n            self.ln_1s.append(tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS))\n            # Multi Head Attention\n            self.mhas.append(MultiHeadAttention(UNITS_DECODER, NUM_HEADS, MHA_DROPOUT_RATIO))\n            # Second Layer Normalisation\n            self.ln_2s.append(tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS))\n            # Multi Layer Perception\n            self.mlps.append(tf.keras.Sequential([\n                tf.keras.layers.Dense(UNITS_DECODER * MLP_RATIO, activation=GELU, kernel_initializer=INIT_GLOROT_UNIFORM, use_bias=False),\n                tf.keras.layers.Dropout(MLP_DROPOUT_RATIO),\n                tf.keras.layers.Dense(UNITS_DECODER, kernel_initializer=INIT_HE_UNIFORM, use_bias=False),\n            ]))\n            \n    def get_causal_attention_mask(self):\n        i = tf.range(N_TARGET_FRAMES)[:, tf.newaxis]\n        j = tf.range(N_TARGET_FRAMES)\n        mask = tf.cast(i >= j, dtype=tf.int32)\n        mask = tf.reshape(mask, (1, N_TARGET_FRAMES, N_TARGET_FRAMES))\n        mult = tf.concat(\n            [tf.expand_dims(1, -1), tf.constant([1, 1], dtype=tf.int32)],\n            axis=0,\n        )\n        mask = tf.tile(mask, mult)\n        mask = tf.cast(mask, tf.float32)\n        return mask\n    \n    def get_attention_mask(self, x_inp):\n        # Attention Mask\n        attention_mask = tf.math.count_nonzero(x_inp, axis=[2], keepdims=True, dtype=tf.int32)\n        attention_mask = tf.math.count_nonzero(attention_mask, axis=[2], keepdims=False)\n        attention_mask = tf.expand_dims(attention_mask, axis=1)\n        attention_mask = tf.expand_dims(attention_mask, axis=1)\n        return attention_mask\n        \n    def call(self, encoder_outputs, phrase, x_inp, training=False):\n        # Batch Size\n        B = tf.shape(encoder_outputs)[0]\n        # Cast to INT32\n        phrase = tf.cast(phrase, tf.int32)\n        # Prepend SOS Token\n        phrase = tf.pad(phrase, [[0,0], [1,0]], constant_values=START_TOKEN, name='prepend_sos_token')\n        # Pad With PAD Token\n        phrase = tf.pad(phrase, [[0,0], [0,N_TARGET_FRAMES-MAX_PHRASE_LENGTH-1]], constant_values=PAD_TOKEN, name='append_pad_token')\n        # Positional Embedding\n        x = self.positional_embedding + self.char_emb(phrase)\n        # Causal Attention\n        x = self.pos_emb_ln(x + self.pos_emb_mha(x, x, x, attention_mask=self.causal_mask))\n        # Attention mask to ignore missing frames\n        attention_mask = self.get_attention_mask(x_inp)\n        # Iterate input over transformer blocks\n        for ln_1, mha, ln_2, mlp in zip(self.ln_1s, self.mhas, self.ln_2s, self.mlps):\n            x = ln_1(x + mha(x, encoder_outputs, encoder_outputs, attention_mask=attention_mask))\n            x = ln_2(x + mlp(x))\n        # Slice 31 Characters\n        x = tf.slice(x, [0, 0, 0], [-1, MAX_PHRASE_LENGTH, -1])\n    \n        return x","metadata":{"papermill":{"duration":0.046582,"end_time":"2023-07-02T11:33:54.014771","exception":false,"start_time":"2023-07-02T11:33:53.968189","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:16.533958Z","iopub.execute_input":"2023-09-24T16:36:16.534628Z","iopub.status.idle":"2023-09-24T16:36:16.555112Z","shell.execute_reply.started":"2023-09-24T16:36:16.534596Z","shell.execute_reply":"2023-09-24T16:36:16.554449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This function is used to generate a causal attention mask for the decoder, ensuring that each position in the sequence can only attend to previous positions.\n\n1. **Inputs**: The function takes a single argument `B`, which represents the batch size. This parameter is used to determine the batch dimension when creating the mask.\n\n2. **Generating the Mask**: The mask is generated using two TensorFlow tensors `i` and `j`, where `i` represents the row indices and `j` represents the column indices. These tensors are created using `tf.range` to produce sequences of integers.\n\n3. **Causal Mask**: The `mask` tensor is created by comparing `i` and `j` using the expression `i >= j`. This results in a tensor where each element is `True` if the row index is greater than or equal to the column index (causal mask condition), and `False` otherwise.\n\n4. **Reshaping the Mask**: The mask is reshaped to have shape `(1, N_TARGET_FRAMES, N_TARGET_FRAMES)`, adding a singleton dimension at the beginning. This reshaping is necessary for broadcasting the mask during subsequent operations.\n\n5. **Multiplying Batch Dimension**: The mask is multiplied along its first axis (batch dimension) by `B` to account for different batch sizes. This is done by concatenating `[1, 1]` with `tf.expand_dims(B, -1)` and applying element-wise multiplication. The result is a mask with shape `(B, N_TARGET_FRAMES, N_TARGET_FRAMES)`.\n\n6. **Data Type Conversion**: The final mask is cast to a float32 data type to ensure compatibility with other TensorFlow operations.\n\n7. **Returning the Mask**: The function returns the generated causal attention mask, which can be used in the decoder to enforce causality.\n\nThis function can be called with the batch size as an argument to generate a causal attention mask tailored to the batch size used during decoding in your model.","metadata":{}},{"cell_type":"code","source":"# Causal Attention to make decoder not attent to future characters which it needs to predict\ndef get_causal_attention_mask(B):\n    i = tf.range(N_TARGET_FRAMES)[:, tf.newaxis]\n    j = tf.range(N_TARGET_FRAMES)\n    mask = tf.cast(i >= j, dtype=tf.int32)\n    mask = tf.reshape(mask, (1, N_TARGET_FRAMES, N_TARGET_FRAMES))\n    mult = tf.concat(\n        [tf.expand_dims(B, -1), tf.constant([1, 1], dtype=tf.int32)],\n        axis=0,\n    )\n    mask = tf.tile(mask, mult)\n    mask = tf.cast(mask, tf.float32)\n    return mask","metadata":{"papermill":{"duration":0.094639,"end_time":"2023-07-02T11:33:54.137348","exception":false,"start_time":"2023-07-02T11:33:54.042709","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:16.556523Z","iopub.execute_input":"2023-09-24T16:36:16.557227Z","iopub.status.idle":"2023-09-24T16:36:16.568280Z","shell.execute_reply.started":"2023-09-24T16:36:16.557194Z","shell.execute_reply":"2023-09-24T16:36:16.567385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see how to create a causal attention mask for a specific batch size `B`, and it correctly generates the mask. \n\n1. `B = 1`: This sets the batch size to 1, indicating that you are generating a causal attention mask for a single example.\n\n2. `i = tf.range(N_TARGET_FRAMES)[:, tf.newaxis]`: This creates a tensor `i` containing a range of integers from 0 to `N_TARGET_FRAMES - 1`. The `[:, tf.newaxis]` operation adds a new axis to `i`, converting it into a column vector.\n\n3. `j = tf.range(N_TARGET_FRAMES)`: This creates a tensor `j` similar to `i`, representing another range of integers.\n\n4. `mask = tf.cast(i >= j, dtype=tf.int32)`: Here, you compare each element of `i` with each element of `j` using the `>=` operator. This results in a boolean tensor where each element is `True` if the corresponding element in `i` is greater than or equal to the corresponding element in `j`, and `False` otherwise. The `tf.cast` function is then used to cast this boolean tensor to `tf.int32`, effectively converting `True` to 1 and `False` to 0.\n\n5. `tf.expand_dims(B, -1)`: This creates a tensor with the value of `B` and adds a new axis at the end of the tensor. The `-1` argument indicates that the new axis should be added at the last dimension.\n\n6. `mask = tf.reshape(mask, (1, N_TARGET_FRAMES, N_TARGET_FRAMES))`: The `mask` tensor is reshaped to have the shape `(1, N_TARGET_FRAMES, N_TARGET_FRAMES)`. This reshaping adds a singleton dimension at the beginning, effectively creating a batch dimension with size 1.\n\n7. `mult = tf.concat([tf.expand_dims(B, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0)`: This code concatenates two tensors along the first dimension (axis 0). The first tensor is the result of `tf.expand_dims(B, -1)`, which has a shape of `(1, 1)` because of the added singleton dimension. The second tensor is created using `tf.constant` and has the shape `(2,)`. The `mult` tensor is the result of this concatenation, and it has a shape of `(2, 1)`.\n\n8. `mask = tf.tile(mask, mult)`: The `tf.tile` function is used to replicate the `mask` tensor along dimensions specified by the `mult` tensor. In this case, it replicates the mask along the batch dimension (axis 0) by a factor of `B`, which is 1 in this case.\n\n9. `mask = tf.cast(mask, tf.float32)`: Finally, the mask is cast to the `tf.float32` data type to ensure that it contains floating-point values.\n\nThe resulting `mask` tensor is a causal attention mask suitable for a batch size of 1 and can be used in your model for causal self-attention.","metadata":{}},{"cell_type":"code","source":"B = 1\ni = tf.range(N_TARGET_FRAMES)[:, tf.newaxis]\nj = tf.range(N_TARGET_FRAMES)\n\nmask = tf.cast(i >= j, dtype=tf.int32)\ntf.expand_dims(B, -1)\nmask = tf.reshape(mask, (1, N_TARGET_FRAMES, N_TARGET_FRAMES)) # reshape [128,128] to [1,128,128]  \nmult = tf.concat(\n        [tf.expand_dims(B, -1), tf.constant([1, 1], dtype=tf.int32)],\n        axis=0,)\nmask = tf.tile(mask, mult)\nmask = tf.cast(mask, tf.float32)\n\nmask","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:36:16.571466Z","iopub.execute_input":"2023-09-24T16:36:16.571732Z","iopub.status.idle":"2023-09-24T16:36:16.635386Z","shell.execute_reply.started":"2023-09-24T16:36:16.571704Z","shell.execute_reply":"2023-09-24T16:36:16.634422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TopKAccuracy\n\nThe `TopKAccuracy` class is a custom metric for measuring top-k accuracy for multi-dimensional output. Here's an explanation of how this metric works:\n\n1. **Initialization**: The `__init__` method is used to initialize the metric. It takes two arguments: `k` and `**kwargs`. The `k` argument specifies the value of k for the top-k accuracy calculation. The `**kwargs` argument allows additional keyword arguments to be passed when initializing the metric. The metric is named based on the value of `k`.\n\n2. **Metric Initialization**: Inside the `__init__` method, a `SparseTopKCategoricalAccuracy` metric is created and stored in `self.top_k_acc`. This sub-metric is part of TensorFlow and is designed to compute top-k categorical accuracy for sparse targets. It calculates the accuracy of the top-k predictions against the true labels.\n\n3. **`update_state` Method**: The `update_state` method is used to update the metric's state based on the true labels (`y_true`) and the predicted probabilities (`y_pred`). It also accepts an optional `sample_weight` argument, which is not used in this implementation.\n\n    - `y_true` and `y_pred` are reshaped to have the same shape before further processing.\n    \n    - `character_idxs` is computed using `tf.where` to find the indices where `y_true` is less than `N_UNIQUE_CHARACTERS0`. This step is necessary to filter out any special tokens or padding tokens from the calculation, ensuring that only valid character indices are considered for accuracy.\n    \n    - The `tf.gather` function is then used to extract the relevant rows from `y_true` and `y_pred` based on the indices in `character_idxs`. This step effectively filters out any unwanted tokens from both the true labels and predicted probabilities.\n    \n    - Finally, the `update_state` method of the `SparseTopKCategoricalAccuracy` sub-metric (`self.top_k_acc`) is called with the filtered `y_true` and `y_pred` to update the top-k accuracy.\n\n4. **`result` Method**: The `result` method is implemented to return the top-k accuracy as computed by the `SparseTopKCategoricalAccuracy` sub-metric (`self.top_k_acc`). It provides the final top-k accuracy value.\n\n5. **`reset_state` Method**: The `reset_state` method is used to reset the state of the `SparseTopKCategoricalAccuracy` sub-metric (`self.top_k_acc`). It ensures that the metric can be used for a new batch of data without carrying over state information from previous batches.\n\nThe `TopKAccuracy` metric is a custom wrapper around the `SparseTopKCategoricalAccuracy` metric from TensorFlow. It calculates top-k accuracy for multi-dimensional output by filtering out unwanted tokens and then using the sub-metric to compute the accuracy based on the filtered data.","metadata":{"papermill":{"duration":0.024935,"end_time":"2023-07-02T11:33:54.189242","exception":false,"start_time":"2023-07-02T11:33:54.164307","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# TopK accuracy for multi dimensional output\nclass TopKAccuracy(tf.keras.metrics.Metric):\n    def __init__(self, k, **kwargs):\n        super(TopKAccuracy, self).__init__(name=f'top{k}acc', **kwargs)\n        self.top_k_acc = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=k)\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.reshape(y_true, [-1])\n        y_pred = tf.reshape(y_pred, [-1, N_UNIQUE_CHARACTERS])\n        character_idxs = tf.where(y_true < N_UNIQUE_CHARACTERS0)\n        y_true = tf.gather(y_true, character_idxs, axis=0)\n        y_pred = tf.gather(y_pred, character_idxs, axis=0)\n        self.top_k_acc.update_state(y_true, y_pred)\n\n    def result(self):\n        return self.top_k_acc.result()\n    \n    def reset_state(self):\n        self.top_k_acc.reset_state()","metadata":{"papermill":{"duration":0.037227,"end_time":"2023-07-02T11:33:54.251228","exception":false,"start_time":"2023-07-02T11:33:54.214001","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:16.636761Z","iopub.execute_input":"2023-09-24T16:36:16.637083Z","iopub.status.idle":"2023-09-24T16:36:16.645275Z","shell.execute_reply.started":"2023-09-24T16:36:16.637052Z","shell.execute_reply":"2023-09-24T16:36:16.644311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss Weights\n\n","metadata":{"papermill":{"duration":0.024945,"end_time":"2023-07-02T11:33:54.301243","exception":false,"start_time":"2023-07-02T11:33:54.276298","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Here, we are creating an array `loss_weights` to specify the loss weights for different tokens in your model's output.\n\n1. **Initialization**: You create an array `loss_weights` with a length of `N_UNIQUE_CHARACTERS`, which appears to be the number of unique tokens or characters in your model's output.\n\n2. **Set Initial Weights to 1**: Initially, you set all the elements of the `loss_weights` array to 1. This means that all tokens in the model's output will have the same weight when calculating the loss during training.\n\n3. **Set Pad Token Weight to 0**: You specifically set the weight of the \"PAD_TOKEN\" to 0. This indicates that you want to assign zero weight to the padding tokens in your output. Padding tokens are typically used to pad sequences to a uniform length during batch processing but do not contribute to the actual loss calculation. By setting their weight to 0, you effectively ignore them when computing the loss.\n\nThe idea behind setting different loss weights for tokens is to control the importance of each token when computing the loss during training. Tokens that are more critical or meaningful in your task can be assigned higher weights, while tokens that are less important (e.g., padding tokens) can be assigned lower or zero weights.\n\nKeep in mind that these loss weights are often used in sequence-to-sequence models, especially when dealing with padded sequences, to ensure that the model focuses on learning from the meaningful tokens in the sequence and not the padding.","metadata":{}},{"cell_type":"code","source":"# Create Initial Loss Weights All Set To 1\nloss_weights = np.ones(N_UNIQUE_CHARACTERS, dtype=np.float32)\n# Set Loss Weight Of Pad Token To 0\nloss_weights[PAD_TOKEN] = 0","metadata":{"papermill":{"duration":0.033995,"end_time":"2023-07-02T11:33:54.360318","exception":false,"start_time":"2023-07-02T11:33:54.326323","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:16.646918Z","iopub.execute_input":"2023-09-24T16:36:16.647598Z","iopub.status.idle":"2023-09-24T16:36:16.655213Z","shell.execute_reply.started":"2023-09-24T16:36:16.647565Z","shell.execute_reply":"2023-09-24T16:36:16.654133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sparse Categorical Crossentropy With Label Smoothing¶\n\n\nThe `scce_with_ls` function is a custom loss function for training a neural network, specifically designed for tasks involving sparse categorical cross-entropy with label smoothing.\n\n1. **Filter Pad Tokens**: The first step is to filter out padding tokens from both `y_true` and `y_pred`. Padding tokens are usually used to make sequences equal in length, and they are not relevant for loss calculation. Filtering them out ensures that only actual tokens contribute to the loss.\n\n2. **One-Hot Encoding**: Next, `y_true` is one-hot encoded. In a sparse categorical cross-entropy loss, `y_true` typically contains integer labels indicating the true class indices. One-hot encoding converts these integer labels into one-hot vectors. Each one-hot vector has a length equal to the number of unique classes (`N_UNIQUE_CHARACTERS` in this case), and it has a 1 in the position corresponding to the true class and 0s elsewhere.\n\n3. **Categorical Cross-Entropy with Label Smoothing**: With one-hot encoded `y_true`, the function computes the categorical cross-entropy loss (`tf.keras.losses.categorical_crossentropy`) between the one-hot encoded true labels (`y_true`) and the predicted probabilities (`y_pred`). The `label_smoothing` parameter is set to 0.25, which means that label smoothing is applied during the loss calculation. Label smoothing is a regularization technique that prevents the model from becoming too confident about its predictions and encourages it to have softer, more uniform probability distributions over classes.\n\n4. **Reduce Mean**: Finally, the computed loss values are averaged across all non-padding tokens to obtain the final loss value. This loss value can be used to update the neural network's weights during training.\n\nIn summary, `scce_with_ls` is a custom loss function designed for tasks where you have sparse categorical cross-entropy as the loss function and want to apply label smoothing to encourage more robust and generalized predictions from your model. It filters out padding tokens, one-hot encodes the true labels, applies label smoothing during the loss calculation, and computes the mean loss across non-padding tokens.","metadata":{"papermill":{"duration":0.024662,"end_time":"2023-07-02T11:33:54.409907","exception":false,"start_time":"2023-07-02T11:33:54.385245","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# source:: https://stackoverflow.com/questions/60689185/label-smoothing-for-sparse-categorical-crossentropy\ndef scce_with_ls(y_true, y_pred):\n    # Filter Pad Tokens\n    idxs = tf.where(y_true != PAD_TOKEN)\n    y_true = tf.gather_nd(y_true, idxs)\n    y_pred = tf.gather_nd(y_pred, idxs)\n    # One Hot Encode Sparsely Encoded Target Sign\n    y_true = tf.cast(y_true, tf.int32)\n    y_true = tf.one_hot(y_true, N_UNIQUE_CHARACTERS, axis=1)\n    # Categorical Crossentropy with native label smoothing support\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, label_smoothing=0.25, from_logits=True)\n    loss = tf.math.reduce_mean(loss)\n    return loss","metadata":{"papermill":{"duration":0.034321,"end_time":"2023-07-02T11:33:54.468872","exception":false,"start_time":"2023-07-02T11:33:54.434551","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:16.657311Z","iopub.execute_input":"2023-09-24T16:36:16.657674Z","iopub.status.idle":"2023-09-24T16:36:16.668594Z","shell.execute_reply.started":"2023-09-24T16:36:16.657615Z","shell.execute_reply":"2023-09-24T16:36:16.667289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\n","metadata":{"papermill":{"duration":0.025057,"end_time":"2023-07-02T11:33:54.518693","exception":false,"start_time":"2023-07-02T11:33:54.493636","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"We are defining the input layers and processing the inputs through the Encoder component of a Transformer-based model.\n\n1. **Input Layers**: You define two input layers:\n   - `frames_inp`: This input layer represents the input frames, where `N_TARGET_FRAMES` is the number of target frames, and `N_COLS` is the number of columns in the input data.\n   - `phrase_inp`: This input layer represents the input phrase, where `MAX_PHRASE_LENGTH` is the maximum length of the phrase.\n\n2. **Frames Input Processing**:\n   - `x = frames_inp`: You initialize `x` with the `frames_inp` input tensor.\n\n3. **Masking Layer**:\n   - `x = tf.keras.layers.Masking(mask_value=0.0, input_shape=(N_TARGET_FRAMES, N_COLS))(x)`: Here, you apply a masking layer to `x`. This layer masks out values equal to `0.0`, which is often used to represent missing or padding values in sequences. The `input_shape` parameter specifies the expected shape of the input data.\n\n4. **Embedding Layer**:\n   - `x = Embedding()(x)`: You pass the masked input `x` through an `Embedding` layer. This layer applies embeddings to the input data, as explained in a previous code section. It includes positional embeddings and processes the dominant hand landmarks.\n\n5. **Encoder Transformer Blocks**:\n   - `x = Encoder(NUM_BLOCKS_ENCODER)(x, frames_inp)`: This step involves processing the embedded input data through an Encoder component composed of multiple Transformer blocks. You pass `x` as the input data and `frames_inp` as additional information. The Encoder applies self-attention mechanisms and feed-forward layers to capture contextual information from the input frames. The resulting `x` represents the encoded information after passing through the Encoder.\n\n6. **Output Shape**:\n   - After processing through the Encoder, you print the shape of `x`, which represents the output shape post-encoding, and the value of `UNITS_DECODER`, which indicates the size of the final output.\n\nThis part is initial processing of input data and the application of the Encoder component in a Transformer-based model. This is a common setup for sequence-to-sequence tasks, where the input frames are encoded into a meaningful representation before being used for generating an output sequence (e.g., a translation or prediction task).","metadata":{}},{"cell_type":"code","source":"# Inputs\nframes_inp = tf.keras.layers.Input([N_TARGET_FRAMES, N_COLS], dtype=tf.float32, name='frames')\nphrase_inp = tf.keras.layers.Input([MAX_PHRASE_LENGTH], dtype=tf.int32, name='phrase')\n# Frames\nx = frames_inp\n\n# Masking to eliminate NaN values in frames\nx = tf.keras.layers.Masking(mask_value=0.0, input_shape=(N_TARGET_FRAMES, N_COLS))(x)\nprint(f' Input shape: {x.shape}')\n# Embedding\nx = Embedding()(x)\n\nprint(f' Shape post Embedding: {x.shape}')\n    \n# Encoder Transformer Blocks\nx = Encoder(NUM_BLOCKS_ENCODER)(x, frames_inp)\n\nprint(f' Shape post encoder: {x.shape}')\nprint(f' Units Decoder: {UNITS_DECODER}')","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:36:16.670827Z","iopub.execute_input":"2023-09-24T16:36:16.671311Z","iopub.status.idle":"2023-09-24T16:36:18.474303Z","shell.execute_reply.started":"2023-09-24T16:36:16.671223Z","shell.execute_reply":"2023-09-24T16:36:18.473301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we define the Decoder component of a Transformer-based model.\n\n1. **Decoder Blocks**:\n   - `x = Decoder(NUM_BLOCKS_DECODER)(x, phrase_inp, frames_inp)`: Here, we are using the Decoder component, which consists of multiple Transformer blocks. We pass the following inputs:\n     - `x`: The output from the Encoder, representing the encoded information of the input frames.\n     - `phrase_inp`: The input phrase, which serves as the target sequence that the model aims to generate.\n     - `frames_inp`: This input is also provided to the Decoder, but its purpose is not explicitly mentioned in this code snippet.\n   - The Decoder processes the encoded input `x` and generates an output sequence. The Decoder blocks include self-attention mechanisms and feed-forward layers designed to generate sequences based on the provided inputs.\n\n2. **Output Shape**:\n   - `print(f'Shape post Decoder: {x.shape}')`: After processing through the Decoder, we print the shape of `x`. This shape represents the output of the Decoder, which is expected to have a shape related to the target sequence length and the number of unique characters in the output vocabulary.\n\n3. **Number of Unique Characters**:\n   - `print(f'No. of unique characters: {N_UNIQUE_CHARACTERS}')`: We print the number of unique characters in the output vocabulary. This is an important value as it indicates the size of the output space, which typically corresponds to the number of different tokens or characters that the model can generate.\n\n4. **Classifier**:\n   - `x = tf.keras.Sequential([...], name='classifier')(x)`: After the Decoder, you apply a sequence of operations to the output `x`. These operations include dropout and a dense layer. The dense layer serves as a classifier, with the number of output neurons equal to the number of unique characters (`N_UNIQUE_CHARACTERS`). This layer essentially performs a mapping from the Decoder's output to the space of possible output characters.\n\n5. **Outputs**:\n   - `outputs = x`: The final output of the model is assigned to `outputs`. This output represents the model's predictions for the target sequence.\n\nThe Decoder in a sequence-to-sequence model like this one plays a crucial role in generating the output sequence based on the information encoded from the input frames and the provided target phrase. The output is typically a sequence of tokens or characters, and the classifier helps map the intermediate representation to this output sequence.","metadata":{}},{"cell_type":"code","source":"# Decoder\nx = Decoder(NUM_BLOCKS_DECODER)(x, phrase_inp, frames_inp)\nprint(f'Shape post Decoder: {x.shape}')    \nprint(f'No. of unique characters: {N_UNIQUE_CHARACTERS}')\n# Classifier\nx = tf.keras.Sequential([\n    # Dropout\n    tf.keras.layers.Dropout(CLASSIFIER_DROPOUT_RATIO), # CLASSIFIER_DROPOUT_RATIO = 0.1\n    # Output Neurons: 62 classes (different tokens)\n    tf.keras.layers.Dense(N_UNIQUE_CHARACTERS, activation=tf.keras.activations.linear,\n                          kernel_initializer=INIT_HE_UNIFORM, use_bias=False),\n    ], name='classifier')(x)\n    \noutputs = x","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:36:18.475625Z","iopub.execute_input":"2023-09-24T16:36:18.477524Z","iopub.status.idle":"2023-09-24T16:36:19.892131Z","shell.execute_reply.started":"2023-09-24T16:36:18.477487Z","shell.execute_reply":"2023-09-24T16:36:19.891133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A deep learning model is defined for a sequence-to-sequence task.\n\n1. **Model Definition**: A TensorFlow/Keras model is created to handle two types of input: `frames_inp` for frames data and `phrase_inp` for phrase data. The model produces an output called `outputs`. The objective is to map sequences of frames and phrases to sequences of characters.\n\n2. **Loss Function**: A custom loss function named `scce_with_ls` is defined. This loss function combines sparse categorical cross-entropy with label smoothing. It's employed to compute the loss during the model training process. Label smoothing is a regularization technique that smooths the target labels to improve the model's generalization.\n\n3. **Optimizer Configuration**: The optimizer for training the model is set up. Specifically, the Rectified Adam optimizer (`tfa.optimizers.RectifiedAdam`) is used with a smoothness threshold of 4. Rectified Adam is an adaptive learning rate optimizer that adjusts learning rates based on the gradient history. Additionally, the Lookahead optimizer (`tfa.optimizers.Lookahead`) is applied to the Rectified Adam optimizer. Lookahead is a technique that can enhance training convergence.\n\n4. **Evaluation Metrics**: A list of evaluation metrics is specified to monitor the model's performance during training. These metrics assess how effectively the model is learning. Two metrics are included:\n   - `TopKAccuracy(1)`: This metric calculates the top-1 accuracy, which measures the frequency of the correct token being the model's top prediction for a given sequence.\n   - `TopKAccuracy(5)`: This metric computes the top-5 accuracy, which measures the frequency of the correct token being among the top 5 predictions made by the model.\n\n5. **Model Compilation**: The model is compiled by defining its loss function, optimizer, evaluation metrics, and loss weights. The loss weights assign varying importance to different classes, with a weight of 0 assigned to the `PAD_TOKEN`. This ensures that the model does not focus on padding tokens during training.\n\n6. **Model Summary**: A summary of the model's architecture is displayed. This summary provides comprehensive information about each layer in the model, including layer names, output shapes, and the number of trainable parameters. It serves as a useful way to inspect the model's structure and complexity.\n\nOnce the model is compiled, it is ready for training. During training, it optimizes its weights to minimize the defined loss function while monitoring the specified evaluation metrics. The ultimate goal is to create a model that accurately maps input frames and phrases to character sequences.","metadata":{}},{"cell_type":"code","source":"# Create Tensorflow Model\nmodel = tf.keras.models.Model(inputs=[frames_inp, phrase_inp], outputs=outputs)\n    \n# Categorical Crossentropy Loss With Label Smoothing\nloss = scce_with_ls\n    \n# Adam Optimizer\noptimizer = tfa.optimizers.RectifiedAdam(sma_threshold=4)\noptimizer = tfa.optimizers.Lookahead(optimizer, sync_period=5)\n\n# TopK Metrics\nmetrics = [\n        TopKAccuracy(1),\n        TopKAccuracy(5),]\n    \nmodel.compile(\n    loss=loss,\n    optimizer=optimizer,\n    metrics=metrics,\n    loss_weights=loss_weights,\n    )\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T16:36:19.893607Z","iopub.execute_input":"2023-09-24T16:36:19.893939Z","iopub.status.idle":"2023-09-24T16:36:19.975613Z","shell.execute_reply.started":"2023-09-24T16:36:19.893906Z","shell.execute_reply":"2023-09-24T16:36:19.974669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `get_model` function is a convenient way to create and configure a deep learning model for a specific sequence-to-sequence task.\n\n1. **Input Layers**: The function starts by defining two input layers:\n   - `frames_inp`: This layer is used for frames data and expects input sequences with a shape of `[N_TARGET_FRAMES, N_COLS]`, where `N_TARGET_FRAMES` represents the number of target frames and `N_COLS` is the number of columns in each frame.\n   - `phrase_inp`: This layer is designed for phrase data and expects input sequences with a maximum length of `MAX_PHRASE_LENGTH`.\n\n2. **Frames Data Preprocessing**:\n   - Masking Layer: A masking layer is applied to the frames input to handle any NaN (Not-a-Number) values in the data. The `mask_value` parameter is set to 0.0, indicating that 0.0 is treated as a masked value.\n\n3. **Embedding Layer**: The frames data is passed through an embedding layer created using the `Embedding` class. This layer performs data normalization and landmark embedding to capture information from the frames.\n\n4. **Encoder Transformer Blocks**: The normalized and embedded frames data is then passed through an encoder, which consists of multiple transformer blocks. These blocks apply multi-head self-attention to capture contextual information from the frames. The number of blocks is determined by the `NUM_BLOCKS_ENCODER` parameter.\n\n5. **Decoder**: After encoding the frames, the function proceeds to the decoder, which is also composed of multiple transformer blocks. These blocks handle the generation of character sequences based on the encoded frames and input phrases. The number of decoder blocks is determined by the `NUM_BLOCKS_DECODER` parameter.\n\n6. **Classifier Layer**: Following the decoder, a classifier layer is added. This layer includes dropout regularization with a dropout rate specified by `CLASSIFIER_DROPOUT_RATIO`. It's responsible for producing the final output, which is a sequence of characters. The number of output neurons in this layer matches the number of unique characters (`N_UNIQUE_CHARACTERS`) in the dataset.\n\n7. **Model Compilation**: The function compiles the model with the following configurations:\n   - Loss Function: It uses the custom loss function `scce_with_ls`, which combines sparse categorical cross-entropy with label smoothing. This loss function encourages the model to make more confident predictions.\n   - Optimizer: The Rectified Adam optimizer (`tfa.optimizers.RectifiedAdam`) is employed with a smoothness threshold of 4. Additionally, the Lookahead optimizer (`tfa.optimizers.Lookahead`) is applied to enhance training convergence.\n   - Metrics: Two evaluation metrics are specified:\n     - `TopKAccuracy(1)`: Measures the top-1 accuracy, indicating how often the correct token is the model's top prediction.\n     - `TopKAccuracy(5)`: Calculates the top-5 accuracy, indicating the frequency of the correct token appearing among the top 5 model predictions.\n   - Loss Weights: The function assigns different loss weights to different classes, with a weight of 0 assigned to the `PAD_TOKEN` class. This ensures that padding tokens are not emphasized during training.\n\n8. **Model Creation**: Finally, the function creates a TensorFlow/Keras model with the specified inputs, outputs, loss, optimizer, metrics, and loss weights. The configured model is returned by the function.\n\nOne can call this `get_model` function to obtain a pre-configured model for the sequence-to-sequence task.","metadata":{}},{"cell_type":"code","source":"def get_model():\n    # Inputs\n    frames_inp = tf.keras.layers.Input([N_TARGET_FRAMES, N_COLS], dtype=tf.float32, name='frames')\n    phrase_inp = tf.keras.layers.Input([MAX_PHRASE_LENGTH], dtype=tf.int32, name='phrase')\n    # Frames\n    x = frames_inp\n\n    # Masking\n    x = tf.keras.layers.Masking(mask_value=0.0, input_shape=(N_TARGET_FRAMES, N_COLS))(x)\n    \n    # Embedding\n    x = Embedding()(x)\n    \n    # Encoder Transformer Blocks\n    x = Encoder(NUM_BLOCKS_ENCODER)(x, frames_inp)\n    \n    # Decoder\n    x = Decoder(NUM_BLOCKS_DECODER)(x, phrase_inp, frames_inp)\n    \n    # Classifier\n    x = tf.keras.Sequential([\n        # Dropout\n        tf.keras.layers.Dropout(CLASSIFIER_DROPOUT_RATIO),\n        # Output Neurons\n        tf.keras.layers.Dense(N_UNIQUE_CHARACTERS, activation=tf.keras.activations.linear, kernel_initializer=INIT_HE_UNIFORM, use_bias=False),\n    ], name='classifier')(x)\n    \n    outputs = x\n    \n    # Create Tensorflow Model\n    model = tf.keras.models.Model(inputs=[frames_inp, phrase_inp], outputs=outputs)\n    \n    # Categorical Crossentropy Loss With Label Smoothing\n    loss = scce_with_ls\n    \n    # Adam Optimizer\n    optimizer = tfa.optimizers.RectifiedAdam(sma_threshold=4)\n    optimizer = tfa.optimizers.Lookahead(optimizer, sync_period=5)\n\n    # TopK Metrics\n    metrics = [\n        TopKAccuracy(1),\n        TopKAccuracy(5),\n    ]\n    \n    model.compile(\n        loss=loss,\n        optimizer=optimizer,\n        metrics=metrics,\n        loss_weights=loss_weights,\n    )\n    \n    return model","metadata":{"papermill":{"duration":0.03826,"end_time":"2023-07-02T11:33:54.581784","exception":false,"start_time":"2023-07-02T11:33:54.543524","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:19.977003Z","iopub.execute_input":"2023-09-24T16:36:19.977609Z","iopub.status.idle":"2023-09-24T16:36:19.988019Z","shell.execute_reply.started":"2023-09-24T16:36:19.977574Z","shell.execute_reply":"2023-09-24T16:36:19.986914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nmodel = get_model()","metadata":{"papermill":{"duration":3.159973,"end_time":"2023-07-02T11:33:57.826278","exception":false,"start_time":"2023-07-02T11:33:54.666305","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:19.989493Z","iopub.execute_input":"2023-09-24T16:36:19.989902Z","iopub.status.idle":"2023-09-24T16:36:22.865450Z","shell.execute_reply.started":"2023-09-24T16:36:19.989870Z","shell.execute_reply":"2023-09-24T16:36:22.864477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot model summary\nmodel.summary(expand_nested=True)","metadata":{"papermill":{"duration":0.237637,"end_time":"2023-07-02T11:33:58.088939","exception":false,"start_time":"2023-07-02T11:33:57.851302","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:22.866757Z","iopub.execute_input":"2023-09-24T16:36:22.869154Z","iopub.status.idle":"2023-09-24T16:36:23.021321Z","shell.execute_reply.started":"2023-09-24T16:36:22.869116Z","shell.execute_reply":"2023-09-24T16:36:23.020584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Model Architecture\ntf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True, show_layer_names=True, expand_nested=True, show_layer_activations=True)","metadata":{"papermill":{"duration":0.307642,"end_time":"2023-07-02T11:33:58.432321","exception":false,"start_time":"2023-07-02T11:33:58.124679","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:23.022347Z","iopub.execute_input":"2023-09-24T16:36:23.022693Z","iopub.status.idle":"2023-09-24T16:36:23.370544Z","shell.execute_reply.started":"2023-09-24T16:36:23.022659Z","shell.execute_reply":"2023-09-24T16:36:23.368411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Learning Rate Scheduler\n\nThe `lrfn` (Learning Rate Function) is a function used for scheduling learning rates during training in deep learning models, particularly for training neural networks with cyclical learning rates. Let's break down this function step by step:\n\n- **Inputs**:\n  - `current_step`: The current training step or iteration.\n  - `num_warmup_steps`: The number of warm-up steps during which the learning rate gradually increases.\n  - `lr_max`: The maximum learning rate to be used during training.\n  - `num_cycles`: The number of cycles of learning rate adjustments. Default value is 0.50.\n  - `num_training_steps`: The total number of training steps or iterations, typically representing the number of epochs multiplied by the number of steps per epoch.\n\n- **Warm-up Phase**:\n  - In the beginning of training, for the first `num_warmup_steps` steps, the learning rate is increased gradually to help the model stabilize and converge more quickly. This is useful to prevent large initial weight updates that might destabilize the training process.\n  - There are two methods for warm-up:\n    - `'log'` Method: This method increases the learning rate using a logarithmic scale, where the learning rate decreases exponentially from `lr_max` as `current_step` increases.\n    - Default Method: If the method is not `'log'`, it uses a method where the learning rate is increased using an exponential scale. It doubles the learning rate at each step until `current_step` reaches `num_warmup_steps`.\n\n- **Main Training Phase**:\n  - After the warm-up phase, the learning rate scheduling depends on the number of cycles and the progress through the training.\n  - `progress` is a value between 0 and 1 that represents the progress through the training. It's calculated as the fraction of steps completed after the warm-up phase.\n  - The learning rate during the main training phase follows a cyclical pattern. It oscillates between a lower bound and `lr_max` based on the cosine of a fraction of cycles completed.\n\n- **Cosine Annealing**:\n  - The learning rate during the main training phase is adjusted using a cosine annealing schedule. This means that the learning rate decreases smoothly from `lr_max` to a lower bound and then smoothly increases back to `lr_max` in a cyclical manner.\n  - The cosine annealing is controlled by the `math.cos` function, and `num_cycles` determines how many times this cycle occurs during training.\n\n- **Final Learning Rate**:\n  - The final learning rate at any step is determined by combining the warm-up phase (if applicable) and the cosine annealing phase.\n\nThe `lrfn` function is used to schedule learning rates during training. It starts with a warm-up phase to gradually increase the learning rate and then enters a main training phase with cyclical adjustments. This scheduling strategy helps improve training stability and convergence in deep learning models. The specific scheduling method (logarithmic or exponential warm-up, cosine annealing) can be customized based on the requirements of the training task.","metadata":{"papermill":{"duration":0.037744,"end_time":"2023-07-02T11:34:13.521856","exception":false,"start_time":"2023-07-02T11:34:13.484112","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=N_EPOCHS):\n    \n    if current_step < num_warmup_steps:\n        if WARMUP_METHOD == 'log':\n            return lr_max * 0.10 ** (num_warmup_steps - current_step)\n        else:\n            return lr_max * 2 ** -(num_warmup_steps - current_step)\n    else:\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max","metadata":{"papermill":{"duration":0.048796,"end_time":"2023-07-02T11:34:13.609084","exception":false,"start_time":"2023-07-02T11:34:13.560288","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:23.372854Z","iopub.execute_input":"2023-09-24T16:36:23.373622Z","iopub.status.idle":"2023-09-24T16:36:23.384348Z","shell.execute_reply.started":"2023-09-24T16:36:23.373587Z","shell.execute_reply":"2023-09-24T16:36:23.381946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `plot_lr_schedule` function is used to visualize the learning rate schedule during training.\n\n- **Inputs**:\n  - `lr_schedule`: A list of learning rates scheduled for each training step or epoch.\n  - `epochs`: The total number of training epochs.\n\n- **Plotting Learning Rate Schedule**:\n  - The function starts by creating a figure (plot) with a specified size (20 units in width and 10 units in height).\n  - It plots the learning rate schedule as a line plot using `plt.plot`. The `lr_schedule` list is plotted with `None` values added at the beginning and end to avoid plotting the line segments connecting the first and last points to the axis.\n  - It sets the x-axis labels using `np.arange` and `plt.xticks` to label every epoch. If there are many epochs, it only labels every 5 epochs for better readability.\n  - The y-axis limit is increased by 10% of the maximum learning rate for better visualization.\n  - A title is added to the plot, displaying information about the learning rate schedule, including the initial, maximum, and final learning rates.\n  - The learning rates at each epoch are plotted as points on the graph, and the values are annotated next to the points for clarity.\n\n- **X and Y Labels**:\n  - The x-axis is labeled as \"Epoch\" with a specified font size.\n  - The y-axis is labeled as \"Learning Rate\" with a specified font size.\n  - A grid is added to the plot.\n\n- **Displaying the Plot**:\n  - Finally, the plot is displayed using `plt.show()`.\n\nThe purpose of this function is to provide a visual representation of how the learning rate changes during training. This is important for monitoring and debugging the learning rate schedule, ensuring that it behaves as expected throughout training. The plotted graph helps practitioners understand the dynamics of learning rate adjustments and their impact on model training.","metadata":{}},{"cell_type":"code","source":"def plot_lr_schedule(lr_schedule, epochs):\n    fig = plt.figure(figsize=(20, 10))\n    plt.plot([None] + lr_schedule + [None])\n    # X Labels\n    x = np.arange(1, epochs + 1)\n    x_axis_labels = [i if epochs <= 40 or i % 5 == 0 or i == 1 else None for i in range(1, epochs + 1)]\n    plt.xlim([1, epochs])\n    plt.xticks(x, x_axis_labels) # set tick step to 1 and let x axis start at 1\n    \n    # Increase y-limit for better readability\n    plt.ylim([0, max(lr_schedule) * 1.1])\n    \n    # Title\n    schedule_info = f'start: {lr_schedule[0]:.1E}, max: {max(lr_schedule):.1E}, final: {lr_schedule[-1]:.1E}'\n    plt.title(f'Step Learning Rate Schedule, {schedule_info}', size=18, pad=12)\n    \n    # Plot Learning Rates\n    for x, val in enumerate(lr_schedule):\n        if epochs <= 40 or x % 5 == 0 or x is epochs - 1:\n            if x < len(lr_schedule) - 1:\n                if lr_schedule[x - 1] < val:\n                    ha = 'right'\n                else:\n                    ha = 'left'\n            elif x == 0:\n                ha = 'right'\n            else:\n                ha = 'left'\n            plt.plot(x + 1, val, 'o', color='black');\n            offset_y = (max(lr_schedule) - min(lr_schedule)) * 0.02\n            plt.annotate(f'{val:.1E}', xy=(x + 1, val + offset_y), size=12, ha=ha)\n    \n    plt.xlabel('Epoch', size=16, labelpad=5)\n    plt.ylabel('Learning Rate', size=16, labelpad=5)\n    plt.grid()\n    plt.show()\n\n# Learning rate for encoder\nLR_SCHEDULE = [lrfn(step, num_warmup_steps=N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=0.50) for step in range(N_EPOCHS)]\n# Plot Learning Rate Schedule\nplot_lr_schedule(LR_SCHEDULE, epochs=N_EPOCHS)\n# Learning Rate Callback\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=0)","metadata":{"papermill":{"duration":0.885914,"end_time":"2023-07-02T11:34:14.533559","exception":false,"start_time":"2023-07-02T11:34:13.647645","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:23.388071Z","iopub.execute_input":"2023-09-24T16:36:23.388861Z","iopub.status.idle":"2023-09-24T16:36:24.225858Z","shell.execute_reply.started":"2023-09-24T16:36:23.388819Z","shell.execute_reply":"2023-09-24T16:36:24.224969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Weight Decay Callback\n\nThe `WeightDecayCallback` is a custom callback in TensorFlow that is used to update the weight decay (L2 regularization term) based on the learning rate during training.\n\n- **Initialization**:\n  - The constructor (`__init__`) accepts an optional argument `wd_ratio`, which represents the weight decay ratio. It is initialized with a default value (`WD_RATIO`).\n\n- **`on_epoch_begin` Method**:\n  - This method is executed at the beginning of each training epoch.\n  - It calculates the weight decay value based on the current learning rate and the `wd_ratio`.\n  - The weight decay is computed as the product of the learning rate and the `wd_ratio`.\n  - It then prints the current learning rate and weight decay to the console for monitoring.\n\n- **Usage**:\n  - This callback should be used during training by passing it to the `callbacks` parameter when calling the `fit` method on a Keras model.\n\nThis callback allows you to dynamically adjust the weight decay based on the learning rate, which can be beneficial in certain optimization scenarios, especially when using learning rate schedules.","metadata":{}},{"cell_type":"code","source":"# Custom callback to update weight decay with learning rate\nclass WeightDecayCallback(tf.keras.callbacks.Callback):\n    def __init__(self, wd_ratio=WD_RATIO):\n        self.step_counter = 0\n        self.wd_ratio = wd_ratio\n    \n    def on_epoch_begin(self, epoch, logs=None):\n        model.optimizer.weight_decay = model.optimizer.learning_rate * self.wd_ratio\n        print(f'learning rate: {model.optimizer.learning_rate.numpy():.2e}, weight decay: {model.optimizer.weight_decay.numpy():.2e}')","metadata":{"papermill":{"duration":0.051216,"end_time":"2023-07-02T11:34:14.702691","exception":false,"start_time":"2023-07-02T11:34:14.651475","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:24.229956Z","iopub.execute_input":"2023-09-24T16:36:24.232119Z","iopub.status.idle":"2023-09-24T16:36:24.240388Z","shell.execute_reply.started":"2023-09-24T16:36:24.232085Z","shell.execute_reply":"2023-09-24T16:36:24.239568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"papermill":{"duration":0.039868,"end_time":"2023-07-02T11:34:56.591346","exception":false,"start_time":"2023-07-02T11:34:56.551478","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# We manually call `gc.collect()` to release unused memory.\ngc.collect()","metadata":{"papermill":{"duration":0.33862,"end_time":"2023-07-02T11:34:56.969069","exception":false,"start_time":"2023-07-02T11:34:56.630449","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:24.244674Z","iopub.execute_input":"2023-09-24T16:36:24.246512Z","iopub.status.idle":"2023-09-24T16:36:24.591945Z","shell.execute_reply.started":"2023-09-24T16:36:24.246478Z","shell.execute_reply":"2023-09-24T16:36:24.591073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code snippet you provided appears to be part of a larger training process.\n\n- **Conditional Training** (`if TRAIN_MODEL`):\n  - This code block is wrapped in a conditional statement that checks whether the `TRAIN_MODEL` variable is set to `True`. If it is, the model training process will be executed; otherwise, training will be skipped.\n\n- **Clearing GPU Memory** (`tf.keras.backend.clear_session()`):\n  - Before training a new model, it's a good practice to clear the GPU memory to release any resources consumed by previously trained models. The `tf.keras.backend.clear_session()` function is used for this purpose.\n\n- **Getting a Fresh Model** (`model = get_model()`):\n  - A new fresh model is obtained by calling the `get_model()` function. This function likely returns a pre-defined model architecture.\n\n- **Model Summary and Sanity Check**:\n  - The `model.summary()` function is called to print a summary of the model's architecture to the console. This summary provides information about the layers, their shapes, and the total number of parameters in the model.\n  - Additionally, there is a `print('\\n\\n')` statement that adds some separation between the model summary and the training output for readability.\n\n- **Actual Training** (`model.fit()`):\n  - The core training process is executed using the `model.fit()` method. This method trains the model on the training dataset (`train_dataset`) for a specified number of epochs (`N_EPOCHS`).\n  - The `steps_per_epoch` argument specifies the number of batches to process in each epoch.\n  - If a validation dataset is available (`val_dataset`), it can be used to evaluate the model's performance during training. The `validation_data` and `validation_steps` arguments are used for this purpose.\n  - Callbacks, such as the learning rate scheduler (`lr_callback`) and the weight decay callback, can be specified to customize the training process.\n  - The `verbose` argument controls the verbosity of the training output. Depending on the value of `VERBOSE`, it can print training progress information to the console.\n\n- **Impact on the Remaining Code**:\n  - This code block represents the core training process of the model. It initializes a new model, trains it, and monitors its progress. Removing this part of the code would mean that model training does not occur.\n\nOverall, training the model, and its execution depends on the value of `TRAIN_MODEL`. If `TRAIN_MODEL` is `True`, the model training process is executed as described. If it's `False`, training is skipped.","metadata":{}},{"cell_type":"code","source":"if TRAIN_MODEL:\n    # Clear all models in GPU\n    tf.keras.backend.clear_session()\n\n    # Get new fresh model\n    model = get_model()\n\n    # Sanity Check\n    model.summary()\n    print('\\n\\n')\n    # Actual Training\n    history = model.fit(\n            x=train_dataset,\n            steps_per_epoch=TRAIN_STEPS_PER_EPOCH,\n            epochs=N_EPOCHS,\n            # Only used for validation data since training data is a generator\n            validation_data=val_dataset if USE_VAL else None,\n            validation_steps=N_VAL_STEPS_PER_EPOCH if USE_VAL else None,\n            callbacks=[\n                lr_callback,\n                WeightDecayCallback(),\n            ],\n            verbose = VERBOSE,\n        )","metadata":{"papermill":{"duration":12244.208248,"end_time":"2023-07-02T14:59:01.217198","exception":false,"start_time":"2023-07-02T11:34:57.00895","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T16:36:24.596332Z","iopub.execute_input":"2023-09-24T16:36:24.599131Z","iopub.status.idle":"2023-09-24T18:26:16.716718Z","shell.execute_reply.started":"2023-09-24T16:36:24.599095Z","shell.execute_reply":"2023-09-24T18:26:16.715522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Weights\nif LOAD_WEIGHTS: # LOAD_WEIGHTS is set to False\n    model.load_weights('/kaggle/input/aslfr-training-python37/model.h5')\n    print(f'Successfully Loaded Pretrained Weights')","metadata":{"papermill":{"duration":0.069569,"end_time":"2023-07-02T14:59:01.347383","exception":false,"start_time":"2023-07-02T14:59:01.277814","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T18:26:16.718616Z","iopub.execute_input":"2023-09-24T18:26:16.718932Z","iopub.status.idle":"2023-09-24T18:26:16.725182Z","shell.execute_reply.started":"2023-09-24T18:26:16.718906Z","shell.execute_reply":"2023-09-24T18:26:16.724193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save Model Weights\nmodel.save_weights('model.h5')","metadata":{"papermill":{"duration":0.206194,"end_time":"2023-07-02T14:59:01.616525","exception":false,"start_time":"2023-07-02T14:59:01.410331","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T18:26:16.726588Z","iopub.execute_input":"2023-09-24T18:26:16.727113Z","iopub.status.idle":"2023-09-24T18:26:16.855999Z","shell.execute_reply.started":"2023-09-24T18:26:16.727079Z","shell.execute_reply":"2023-09-24T18:26:16.854958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we evaluate the loaded model on the specified dataset and calculate the loss and metrics of the model on that dataset. This step helps us verify that the model is loaded correctly and ready for use. Since `train_dataset` is a generator object, each time we access it, we obtain a new batch of different data. Therefore, we are not evaluating with the same data that we used for training.\n\nThe `evaluate()` function prints three values to the screen in the form of a list:\n\n- Loss (Sparse Categorical Cross Entropy with Label Smoothing in our case): This measures how well the model's predictions fit the true labels during training. A lower loss indicates a better fit of the model to the training data.\n\n- Top-1 Accuracy (TopKAccuracy(1)): This is the top-1 accuracy, also known as the accuracy in the ranking of the highest probability. It represents the fraction of samples in which the model correctly predicts the true class as the class with the highest probability. In other words, it is the accuracy for the case where only the most probable class is taken as the prediction.\n\n- Top-5 Accuracy (TopKAccuracy(5)): This is the top-5 accuracy, which represents the fraction of samples in which the model correctly predicts the true class as one of the five classes with the highest probabilities. In other words, it considers the top five most probable classes and checks if the true class is present in those five classes.","metadata":{"execution":{"iopub.status.busy":"2023-07-26T17:48:41.772628Z","iopub.execute_input":"2023-07-26T17:48:41.77303Z","iopub.status.idle":"2023-07-26T17:48:42.5113Z","shell.execute_reply.started":"2023-07-26T17:48:41.772989Z","shell.execute_reply":"2023-07-26T17:48:42.509845Z"}}},{"cell_type":"code","source":"# Verify Model is Loaded Correctly\nmodel.evaluate(\n    val_dataset if USE_VAL else train_dataset,\n    steps=N_VAL_STEPS_PER_EPOCH if USE_VAL else TRAIN_STEPS_PER_EPOCH,\n    batch_size=BATCH_SIZE,\n    verbose=VERBOSE,\n)","metadata":{"papermill":{"duration":41.357991,"end_time":"2023-07-02T14:59:43.031109","exception":false,"start_time":"2023-07-02T14:59:01.673118","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T18:26:16.857438Z","iopub.execute_input":"2023-09-24T18:26:16.857995Z","iopub.status.idle":"2023-09-24T18:26:27.148663Z","shell.execute_reply.started":"2023-09-24T18:26:16.857959Z","shell.execute_reply":"2023-09-24T18:26:27.147476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Performance","metadata":{"papermill":{"duration":0.056044,"end_time":"2023-07-02T14:59:43.142922","exception":false,"start_time":"2023-07-02T14:59:43.086878","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Levenshtein Distance","metadata":{}},{"cell_type":"markdown","source":"\nLevenshtein Distance, also known as the Edit Distance, is a metric used to measure the similarity or difference between two strings. Specifically, it measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to transform one string into another.\n\nFor example, let's say we have two strings: \"kitten\" and \"sitting.\" The Levenshtein Distance between these two strings is 3 because you can transform \"kitten\" into \"sitting\" with the following three edits:\n\n1. Substitute 'k' with 's': \"sitten\"\n2. Substitute 'e' with 'i': \"sittin\"\n3. Add 'g' at the end: \"sitting\"\n\nLevenshtein Distance is commonly used in various natural language processing tasks, including spell checking, DNA sequence analysis, and text similarity measurement. In the context of text generation or translation, it can be used to evaluate how similar a generated text is to a reference text. Smaller Levenshtein Distance values indicate greater similarity between two strings.\n\nHere we use Levenshtein Distance to compare the model's predicted phrase (output) to the ground truth or reference phrase. This can help you quantify the accuracy of the generated text by measuring the number of edits needed to make the generated text match the reference text.\n\n\nThe `outputs2phrase` function takes model outputs, which are typically one-hot encoded or logits, and converts them into a human-readable string. Here's an explanation of how it works:\n\n- If the `outputs` array has a dimension of 2, it implies that the model outputs are one-hot encoded. In this case, the function uses `np.argmax(outputs, axis=1)` to find the index of the highest value in each row (each row represents a character or token). This index corresponds to the predicted character/token.\n\n- Next, the function iterates through the predicted indices and uses the `num_to_char` dictionary to map each index to its corresponding character/token. It appends these characters/tokens together to form a string.\n\nIn essence, `outputs2phrase` decodes the model's output into a sequence of characters or tokens that represent a predicted phrase or sequence.\n","metadata":{}},{"cell_type":"code","source":"# Output Predictions to string\ndef outputs2phrase(outputs):\n    if outputs.ndim == 2:\n        outputs = np.argmax(outputs, axis=1)\n    \n    return ''.join([num_to_char.get(s, '') for s in outputs])","metadata":{"papermill":{"duration":0.066802,"end_time":"2023-07-02T14:59:43.266519","exception":false,"start_time":"2023-07-02T14:59:43.199717","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T18:26:27.150343Z","iopub.execute_input":"2023-09-24T18:26:27.150984Z","iopub.status.idle":"2023-09-24T18:26:27.157025Z","shell.execute_reply.started":"2023-09-24T18:26:27.150947Z","shell.execute_reply":"2023-09-24T18:26:27.155820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `predict_phrase` function is a TensorFlow function decorated with `@tf.function()`, which is used to create a graph from the Python code for optimized execution. Here's an explanation of how this function works:\n\n1. **Input Preparation**:\n   - The function takes a single argument, `frames`, which is a tensor containing input frames.\n   - It adds a batch dimension to the frames tensor using `tf.expand_dims(frames, axis=0)` to make it compatible with the model's input shape.\n\n2. **Initialization**:\n   - It initializes an empty tensor `phrase` of shape `[1, MAX_PHRASE_LENGTH]` filled with PAD_TOKEN values. This tensor will be used to build the predicted phrase.\n\n3. **Prediction Loop**:\n   - The function enters a loop that iterates for a maximum of `MAX_PHRASE_LENGTH` times. This loop is used to predict each token in the phrase.\n   - Inside the loop, it casts the `phrase` tensor to `int8` data type.\n\n   - It passes the `frames` and `phrase` tensors to the model as inputs using `model(...)`.\n   \n   - It uses `tf.argmax(outputs, axis=2, output_type=tf.int32)` to find the index (token) with the highest probability in the model's output (`outputs`). This predicted token is added to the `phrase` tensor at the appropriate position, effectively extending the predicted phrase.\n   \n   - A mask is created with `tf.range(MAX_PHRASE_LENGTH) < idx + 1`, which evaluates to `True` for positions in the phrase that have been predicted so far (up to the current index).\n   \n   - The `tf.where` function updates the `phrase` tensor: it keeps the original PAD_TOKEN values where the mask is `False` (indicating positions that haven't been predicted yet) and replaces positions where the mask is `True` with the predicted token.\n\n4. **Output Preparation**:\n   - After the loop, the function squeezes the batch dimension from the `phrase` tensor using `tf.squeeze(phrase, axis=0)` to get a tensor of shape `[MAX_PHRASE_LENGTH]`.\n   \n   - It then performs one-hot encoding on this tensor using `tf.one_hot(outputs, N_UNIQUE_CHARACTERS)` to obtain a one-hot encoded representation of the predicted phrase.\n\n5. **Return**:\n   - The function returns the one-hot encoded tensor containing the predicted phrase.\n\nThis function is designed to generate a phrase by iteratively predicting tokens one by one based on the input frames and the model's predictions. It uses a loop to build the phrase, updating the predicted token at each step. The final output is a one-hot encoded representation of the predicted phrase.","metadata":{}},{"cell_type":"code","source":"@tf.function()\ndef predict_phrase(frames):\n    # Add Batch Dimension\n    frames = tf.expand_dims(frames, axis=0)\n    # Start Phrase\n    phrase = tf.fill([1,MAX_PHRASE_LENGTH], PAD_TOKEN)\n\n    for idx in tf.range(MAX_PHRASE_LENGTH):\n        # Cast phrase to int8\n        phrase = tf.cast(phrase, tf.int8)\n        # Predict Next Token\n        outputs = model({\n            'frames': frames,\n            'phrase': phrase,\n        })\n\n        # Add predicted token to input phrase\n        phrase = tf.cast(phrase, tf.int32)\n        phrase = tf.where( # where its True search for max prob, if its False keep Pad token\n            tf.range(MAX_PHRASE_LENGTH) < idx + 1, # create a mask of Trues and Falses\n            tf.argmax(outputs, axis=2, output_type=tf.int32), # search for the max probability\n            phrase,\n        )\n\n    # Squeeze outputs\n    outputs = tf.squeeze(phrase, axis=0) # drop first dimension\n    outputs = tf.one_hot(outputs, N_UNIQUE_CHARACTERS) # one-hot encoding of the numbers\n\n    # Return a dictionary with the output tensor\n    return outputs","metadata":{"papermill":{"duration":0.068395,"end_time":"2023-07-02T14:59:43.392955","exception":false,"start_time":"2023-07-02T14:59:43.32456","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T18:26:27.158649Z","iopub.execute_input":"2023-09-24T18:26:27.159195Z","iopub.status.idle":"2023-09-24T18:26:27.170238Z","shell.execute_reply.started":"2023-09-24T18:26:27.159160Z","shell.execute_reply":"2023-09-24T18:26:27.169319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Levenshtein's distance evaluation on training data\n\nHere we computes Levenshtein distances between predicted and true phrases and organizes the results into a DataFrame.\n\n1. **Initialization**:\n   - The variable `N` is set to 100 if `IS_INTERACTIVE` is true (or 1000 otherwise). This determines how many samples will be processed.\n   - An empty list `LD_TRAIN` is initialized to store dictionaries containing information about each sample's true phrase, predicted phrase, and Levenshtein distance.\n\n2. **Loop Over Data**:\n   - The code iterates over the training data using a `for` loop and the `zip` function, combining input frames (`frames`) and true phrases (`phrase_true`) for each sample.\n   - For each sample, it does the following:\n      - Calls the `predict_phrase` function to generate a predicted phrase based on the input frames. This prediction is converted to a string.\n      - Converts the true phrase from its ordinal representation to a string.\n      - Computes the Levenshtein distance between the predicted and true phrases using the `levenshtein` function (not shown in your provided code).\n\n3. **DataFrame Creation**:\n   - For each sample, a dictionary is created containing the following:\n     - `'phrase_true'`: The true phrase as a string.\n     - `'phrase_pred'`: The predicted phrase as a string.\n     - `'levenshtein_distance'`: The Levenshtein distance between the predicted and true phrases.\n   - Each dictionary is appended to the `LD_TRAIN` list.\n\n4. **Subset Selection**:\n   - If the current index (`idx`) reaches the value of `N`, the loop stops, creating a subset of `N` samples. This is useful in interactive mode to limit the number of processed samples.\n\n5. **DataFrame Conversion**:\n   - The `LD_TRAIN` list of dictionaries is converted into a Pandas DataFrame named `LD_TRAIN_DF`.\n\n6. **Adding Length Column**:\n   - A new column `'len_char'` is added to the DataFrame using the `apply` function. This column stores the length (number of characters) of the true phrases.\n\n7. **Displaying Errors**:\n   - The code displays the first 25 rows of the `LD_TRAIN_DF` DataFrame, showing information about true phrases, predicted phrases, Levenshtein distances, and phrase lengths.\n\nWe processes a subset of training data (controlled by `N`), calculate Levenshtein distances between predicted and true phrases, and presents the results in a DataFrame for further analysis and inspection of errors in phrase predictions.","metadata":{"papermill":{"duration":0.056358,"end_time":"2023-07-02T14:59:43.504839","exception":false,"start_time":"2023-07-02T14:59:43.448481","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Compute Levenstein Distances\ndef get_ld_train():\n    N = 100 if IS_INTERACTIVE else 1000\n    LD_TRAIN = []\n    for idx, (frames, phrase_true) in enumerate(zip(tqdm(X_train, total=N), y_train)):\n        # Predict Phrase and Convert to String\n        phrase_pred = predict_phrase(frames).numpy()\n        phrase_pred = outputs2phrase(phrase_pred)\n        # True Phrase Ordinal to String\n        phrase_true = outputs2phrase(phrase_true)\n        # Add Levenstein Distance\n        LD_TRAIN.append({\n            'phrase_true': phrase_true,\n            'phrase_pred': phrase_pred,\n            'levenshtein_distance': levenshtein(phrase_pred, phrase_true),\n        })\n        # Take subset in interactive mode\n        if idx == N:\n            break\n            \n    # Convert to DataFrame\n    LD_TRAIN_DF = pd.DataFrame(LD_TRAIN)\n    \n    return LD_TRAIN_DF","metadata":{"papermill":{"duration":0.071676,"end_time":"2023-07-02T14:59:43.633777","exception":false,"start_time":"2023-07-02T14:59:43.562101","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T18:26:27.172109Z","iopub.execute_input":"2023-09-24T18:26:27.172460Z","iopub.status.idle":"2023-09-24T18:26:27.183487Z","shell.execute_reply.started":"2023-09-24T18:26:27.172429Z","shell.execute_reply":"2023-09-24T18:26:27.182529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LD_TRAIN_DF = get_ld_train()\n\n# add column to see the length of the true phrase\nLD_TRAIN_DF['len_char'] = LD_TRAIN_DF['phrase_true'].apply(lambda x: len(x))\n\n# Display Errors\ndisplay(LD_TRAIN_DF.head(25))","metadata":{"papermill":{"duration":126.418133,"end_time":"2023-07-02T15:01:50.108598","exception":false,"start_time":"2023-07-02T14:59:43.690465","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T18:26:27.186651Z","iopub.execute_input":"2023-09-24T18:26:27.186941Z","iopub.status.idle":"2023-09-24T18:28:24.378824Z","shell.execute_reply.started":"2023-09-24T18:26:27.186917Z","shell.execute_reply":"2023-09-24T18:28:24.377829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we calculate the distribution of Levenshtein distances in the training set and create a bar chart to visualize how these distances are distributed in the dataset.","metadata":{}},{"cell_type":"code","source":"# Value Counts\nLD_TRAIN_VC = dict([(i, 0) for i in range(LD_TRAIN_DF['levenshtein_distance'].max()+1)])\nfor ld in LD_TRAIN_DF['levenshtein_distance']:\n    LD_TRAIN_VC[ld] += 1\n\nplt.figure(figsize=(15,8))\npd.Series(LD_TRAIN_VC).plot(kind='bar', width=1)\nplt.title(f'Train Levenstein Distance Distribution | Mean: {LD_TRAIN_DF.levenshtein_distance.mean():.4f}')\nplt.xlabel('Levenstein Distance')\nplt.ylabel('Sample Count')\nplt.xlim(-0.50, LD_TRAIN_DF.levenshtein_distance.max()+0.50)\nplt.grid(axis='y')\nplt.savefig('temp.png')\nplt.show()","metadata":{"papermill":{"duration":0.724484,"end_time":"2023-07-02T15:01:50.890212","exception":false,"start_time":"2023-07-02T15:01:50.165728","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T18:28:24.380354Z","iopub.execute_input":"2023-09-24T18:28:24.380710Z","iopub.status.idle":"2023-09-24T18:28:24.992882Z","shell.execute_reply.started":"2023-09-24T18:28:24.380676Z","shell.execute_reply":"2023-09-24T18:28:24.991902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Levenshtein's distance evaluation on Validation data\n\nThis part is similar to the previous evaluation on training data that computes Levenshtein distances between predicted and true phrases. However, it is applied to the validation dataset (X_val and y_val).","metadata":{"papermill":{"duration":0.057603,"end_time":"2023-07-02T15:01:51.005493","exception":false,"start_time":"2023-07-02T15:01:50.94789","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Compute Levenstein Distances\ndef get_ld_val():\n    N = 100 if IS_INTERACTIVE else 1000\n    LD_VAL = []\n    for idx, (frames, phrase_true) in enumerate(zip(tqdm(X_val, total=N), y_val)):\n        # Predict Phrase and Convert to String\n        phrase_pred = predict_phrase(frames).numpy()\n        phrase_pred = outputs2phrase(phrase_pred)\n        # True Phrase Ordinal to String\n        phrase_true = outputs2phrase(phrase_true)\n        # Add Levenstein Distance\n        LD_VAL.append({\n            'phrase_true': phrase_true,\n            'phrase_pred': phrase_pred,\n            'levenshtein_distance': levenshtein(phrase_pred, phrase_true),\n        })\n        # Take subset in interactive mode\n        if idx == N:\n            break\n            \n    # Convert to DataFrame\n    LD_VAL_DF = pd.DataFrame(LD_VAL)\n    \n    return LD_VAL_DF","metadata":{"papermill":{"duration":0.068889,"end_time":"2023-07-02T15:01:51.131182","exception":false,"start_time":"2023-07-02T15:01:51.062293","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T18:28:24.994250Z","iopub.execute_input":"2023-09-24T18:28:24.994703Z","iopub.status.idle":"2023-09-24T18:28:25.002007Z","shell.execute_reply.started":"2023-09-24T18:28:24.994670Z","shell.execute_reply":"2023-09-24T18:28:25.001058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if USE_VAL:\n    LD_VAL_DF = get_ld_val()\n\n    # Display Errors\n    display(LD_VAL_DF.head(25))","metadata":{"papermill":{"duration":0.064943,"end_time":"2023-07-02T15:01:51.25391","exception":false,"start_time":"2023-07-02T15:01:51.188967","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-24T18:28:25.003300Z","iopub.execute_input":"2023-09-24T18:28:25.003830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Value Counts\nif USE_VAL:\n    LD_VAL_VC = dict([(i, 0) for i in range(LD_VAL_DF['levenshtein_distance'].max()+1)])\n    for ld in LD_VAL_DF['levenshtein_distance']:\n        LD_VAL_VC[ld] += 1\n\n    plt.figure(figsize=(15,8))\n    pd.Series(LD_VAL_VC).plot(kind='bar', width=1)\n    plt.title(f'Validation Levenstein Distance Distribution | Mean: {LD_VAL_DF.levenshtein_distance.mean():.4f}')\n    plt.xlabel('Levenstein Distance')\n    plt.ylabel('Sample Count')\n    plt.xlim(0-0.50, LD_VAL_DF.levenshtein_distance.max()+0.50)\n    plt.grid(axis='y')\n    plt.savefig('temp.png')\n    plt.show()","metadata":{"papermill":{"duration":0.068365,"end_time":"2023-07-02T15:01:51.379464","exception":false,"start_time":"2023-07-02T15:01:51.311099","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training History\n\nOnce the model is trained and predictions are created for our data, a function called `plot_history_metric` is defined to plot the evolution of different metrics used in the training.","metadata":{"papermill":{"duration":0.056463,"end_time":"2023-07-02T15:01:51.49247","exception":false,"start_time":"2023-07-02T15:01:51.436007","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def plot_history_metric(metric, f_best=np.argmax, ylim=None, yscale=None, yticks=None):\n    # Only plot when training\n    if not TRAIN_MODEL:\n        return\n    \n    plt.figure(figsize=(20, 10))\n    \n    values = history.history[metric]\n    N_EPOCHS = len(values)\n    val = 'val' in ''.join(history.history.keys())\n    # Epoch Ticks\n    if N_EPOCHS <= 20:\n        x = np.arange(1, N_EPOCHS + 1)\n    else:\n        x = [1, 5] + [10 + 5 * idx for idx in range((N_EPOCHS - 10) // 5 + 1)]\n\n    x_ticks = np.arange(1, N_EPOCHS+1)\n\n    # Validation\n    if val:\n        val_values = history.history[f'val_{metric}']\n        val_argmin = f_best(val_values)\n        plt.plot(x_ticks, val_values, label=f'val')\n\n    # summarize history for accuracy\n    plt.plot(x_ticks, values, label=f'train')\n    argmin = f_best(values)\n    plt.scatter(argmin + 1, values[argmin], color='red', s=75, marker='o', label=f'train_best')\n    if val:\n        plt.scatter(val_argmin + 1, val_values[val_argmin], color='purple', s=75, marker='o', label=f'val_best')\n\n    plt.title(f'Model {metric}', fontsize=24, pad=10)\n    plt.ylabel(metric, fontsize=20, labelpad=10)\n\n    if ylim:\n        plt.ylim(ylim)\n\n    if yscale is not None:\n        plt.yscale(yscale)\n        \n    if yticks is not None:\n        plt.yticks(yticks, fontsize=16)\n\n    plt.xlabel('epoch', fontsize=20, labelpad=10)        \n    plt.tick_params(axis='x', labelsize=8)\n    plt.xticks(x, fontsize=16) # set tick step to 1 and let x axis start at 1\n    plt.yticks(fontsize=16)\n    \n    plt.legend(prop={'size': 10})\n    plt.grid()\n    plt.show()","metadata":{"papermill":{"duration":0.072449,"end_time":"2023-07-02T15:01:51.621731","exception":false,"start_time":"2023-07-02T15:01:51.549282","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history.history.keys() # there's not 'val'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history_metric('loss', f_best=np.argmin)","metadata":{"papermill":{"duration":0.534279,"end_time":"2023-07-02T15:01:52.213146","exception":false,"start_time":"2023-07-02T15:01:51.678867","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history_metric('top1acc', ylim=[0,1], yticks=np.arange(0.0, 1.1, 0.1))","metadata":{"papermill":{"duration":0.542072,"end_time":"2023-07-02T15:01:52.812958","exception":false,"start_time":"2023-07-02T15:01:52.270886","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history_metric('top5acc', ylim=[0,1], yticks=np.arange(0.0, 1.1, 0.1))","metadata":{"papermill":{"duration":0.542339,"end_time":"2023-07-02T15:01:53.417063","exception":false,"start_time":"2023-07-02T15:01:52.874724","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}